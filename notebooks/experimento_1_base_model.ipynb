{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776a75ae",
   "metadata": {},
   "source": [
    "# Aprendizado Federado para reconhecimento de contexto em dispositivos móveis\n",
    "## Experimento 1 - Pré-treinamento usual com uma base de usuários e Aprendizado Federado embarcado partindo do modelo pretreinado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa27159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.8/dist-packages (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48669192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 20:52:10.529068: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: /usr/lib/x86_64-linux-gnu/libcuda.so.1: file too short; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-07-10 20:52:10.529089: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-10 20:52:10.529104: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2022-07-10 20:52:10.529251: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import es_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d6bcd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbcbddc",
   "metadata": {},
   "source": [
    "Inicialmente precisamos separar quais 'labels' usaremos. Aqui, optamos pelas 51 labels que segundo artigo do ExtraSensosy utiliza, de modo a ter bons parâmetros de comparação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9654a2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"labels = ['label:LYING_DOWN',\\n 'label:SITTING',\\n 'label:FIX_walking',\\n 'label:FIX_running',\\n 'label:BICYCLING',\\n 'label:SLEEPING',\\n 'label:LAB_WORK',\\n 'label:IN_CLASS',\\n 'label:IN_A_MEETING',\\n 'label:LOC_main_workplace',\\n 'label:OR_indoors',\\n 'label:OR_outside',\\n 'label:IN_A_CAR',\\n 'label:ON_A_BUS',\\n 'label:DRIVE_-_I_M_THE_DRIVER',\\n 'label:DRIVE_-_I_M_A_PASSENGER',\\n 'label:LOC_home',\\n 'label:FIX_restaurant',\\n 'label:PHONE_IN_POCKET',\\n 'label:OR_exercise',\\n 'label:COOKING',\\n 'label:SHOPPING',\\n 'label:STROLLING',\\n 'label:DRINKING__ALCOHOL_',\\n 'label:BATHING_-_SHOWER',\\n 'label:CLEANING',\\n 'label:DOING_LAUNDRY',\\n 'label:WASHING_DISHES',\\n 'label:WATCHING_TV',\\n 'label:SURFING_THE_INTERNET',\\n 'label:AT_A_PARTY',\\n 'label:AT_A_BAR',\\n 'label:LOC_beach',\\n 'label:SINGING',\\n 'label:TALKING',\\n 'label:COMPUTER_WORK',\\n 'label:EATING',\\n 'label:TOILET',\\n 'label:GROOMING',\\n 'label:DRESSING',\\n 'label:AT_THE_GYM',\\n 'label:STAIRS_-_GOING_UP',\\n 'label:STAIRS_-_GOING_DOWN',\\n 'label:ELEVATOR',\\n 'label:OR_standing',\\n 'label:AT_SCHOOL',\\n 'label:PHONE_IN_HAND',\\n 'label:PHONE_IN_BAG',\\n 'label:PHONE_ON_TABLE',\\n 'label:WITH_CO-WORKERS',\\n 'label:WITH_FRIENDS']\\n\\nlen(labels)\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''labels = ['label:LYING_DOWN',\n",
    " 'label:SITTING',\n",
    " 'label:FIX_walking',\n",
    " 'label:FIX_running',\n",
    " 'label:BICYCLING',\n",
    " 'label:SLEEPING',\n",
    " 'label:LAB_WORK',\n",
    " 'label:IN_CLASS',\n",
    " 'label:IN_A_MEETING',\n",
    " 'label:LOC_main_workplace',\n",
    " 'label:OR_indoors',\n",
    " 'label:OR_outside',\n",
    " 'label:IN_A_CAR',\n",
    " 'label:ON_A_BUS',\n",
    " 'label:DRIVE_-_I_M_THE_DRIVER',\n",
    " 'label:DRIVE_-_I_M_A_PASSENGER',\n",
    " 'label:LOC_home',\n",
    " 'label:FIX_restaurant',\n",
    " 'label:PHONE_IN_POCKET',\n",
    " 'label:OR_exercise',\n",
    " 'label:COOKING',\n",
    " 'label:SHOPPING',\n",
    " 'label:STROLLING',\n",
    " 'label:DRINKING__ALCOHOL_',\n",
    " 'label:BATHING_-_SHOWER',\n",
    " 'label:CLEANING',\n",
    " 'label:DOING_LAUNDRY',\n",
    " 'label:WASHING_DISHES',\n",
    " 'label:WATCHING_TV',\n",
    " 'label:SURFING_THE_INTERNET',\n",
    " 'label:AT_A_PARTY',\n",
    " 'label:AT_A_BAR',\n",
    " 'label:LOC_beach',\n",
    " 'label:SINGING',\n",
    " 'label:TALKING',\n",
    " 'label:COMPUTER_WORK',\n",
    " 'label:EATING',\n",
    " 'label:TOILET',\n",
    " 'label:GROOMING',\n",
    " 'label:DRESSING',\n",
    " 'label:AT_THE_GYM',\n",
    " 'label:STAIRS_-_GOING_UP',\n",
    " 'label:STAIRS_-_GOING_DOWN',\n",
    " 'label:ELEVATOR',\n",
    " 'label:OR_standing',\n",
    " 'label:AT_SCHOOL',\n",
    " 'label:PHONE_IN_HAND',\n",
    " 'label:PHONE_IN_BAG',\n",
    " 'label:PHONE_ON_TABLE',\n",
    " 'label:WITH_CO-WORKERS',\n",
    " 'label:WITH_FRIENDS']\n",
    "\n",
    "len(labels)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba32040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [\n",
    "    'label:OR_standing',\n",
    "    'label:SITTING',\n",
    "    'label:LYING_DOWN',\n",
    "    'label:FIX_running',\n",
    "    'label:FIX_walking',\n",
    "    'label:BICYCLING'\n",
    "]\n",
    "\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4102452",
   "metadata": {},
   "source": [
    "Separamos os 60 usuários em 5 pastas onde cada pasta considera 40 usuários selecionados aleatoriamente para treino e os 20 restantes para teste. Os dados dos 20 usuários são ainda separados em 4 arquivos *.csv* (x_train, x_test, y_train, y_test) e colocados em pastas nomeadas com o uuid do usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aea84e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_index {} [ 0  1  2  4  5  7  9 11 12 13 16 17 18 19 20 21 22 23 24 25 27 28 29 30\n",
      " 31 32 34 35 36 37 38 39 40 41 42 43 46 47 48 49 51 52 53 54 55 57 58 59]\n",
      "test_index {} [ 3  6  8 10 14 15 26 33 44 45 50 56]\n",
      "split {} ['/home/nonroot/sample_data/61976C24-1C50-4355-9C49-AAE44A7D09F6.features_labels.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n",
      "/home/nonroot/es_utils.py:407: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fold_df_train = fold_df_train.append(pd.read_csv(csv))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_index {} [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 18 19 20 21 23 26 28 29\n",
      " 30 31 32 33 34 35 36 37 43 44 45 46 47 48 49 50 51 52 54 55 56 57 58 59]\n",
      "test_index {} [ 9 17 22 24 25 27 38 39 40 41 42 53]\n",
      "split {} ['/home/nonroot/sample_data/61976C24-1C50-4355-9C49-AAE44A7D09F6.features_labels.csv']\n"
     ]
    }
   ],
   "source": [
    "#k_folds = utils.create_k_folds_n_users(5, 3, '/home/wander/OtherProjects/har_flower/sample_data')\n",
    "k_folds = utils.create_k_folds_n_users(5, 40, '/home/nonroot/sample_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e1e8e",
   "metadata": {},
   "source": [
    "Em seguida, para cada pasta, treinamos um modelo de MLP fazendo uma otimização de hiperparâmetros e convertemos em um modelo *tflite*. Os hiperparâmetros otimizados são:\n",
    "\n",
    "- número de camadas escondidas (*hidden layers*): 1 ou 2\n",
    "- número de neurônios em cada camada escondida: 4 a 64, com um passo de tamanho 4\n",
    "- taxa de aprendizado (*learning rate*) do otimizador (*Adaptive Moment Estimation*, **Adam**): 1e-1, 1e-2, 1e-3 ou 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dc2386d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project hypertunning/experimento_1_RandSearchCV/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from hypertunning/experimento_1_RandSearchCV/tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Epoch 1/100\n",
      "4020/4020 [==============================] - 5s 967us/step - loss: 0.1990 - avg_multilabel_BA_2: 0.7295 - val_loss: 0.1695 - val_avg_multilabel_BA_2: 0.7836\n",
      "Epoch 2/100\n",
      "4020/4020 [==============================] - 4s 901us/step - loss: 0.1636 - avg_multilabel_BA_2: 0.7966 - val_loss: 0.1568 - val_avg_multilabel_BA_2: 0.8051\n",
      "Epoch 3/100\n",
      "4020/4020 [==============================] - 4s 881us/step - loss: 0.1559 - avg_multilabel_BA_2: 0.8104 - val_loss: 0.1537 - val_avg_multilabel_BA_2: 0.8144\n",
      "Epoch 4/100\n",
      "4020/4020 [==============================] - 4s 884us/step - loss: 0.1520 - avg_multilabel_BA_2: 0.8174 - val_loss: 0.1485 - val_avg_multilabel_BA_2: 0.8202\n",
      "Epoch 5/100\n",
      "4020/4020 [==============================] - 4s 881us/step - loss: 0.1492 - avg_multilabel_BA_2: 0.8224 - val_loss: 0.1477 - val_avg_multilabel_BA_2: 0.8243\n",
      "Epoch 6/100\n",
      "4020/4020 [==============================] - 4s 878us/step - loss: 0.1470 - avg_multilabel_BA_2: 0.8260 - val_loss: 0.1471 - val_avg_multilabel_BA_2: 0.8272\n",
      "Epoch 7/100\n",
      "4020/4020 [==============================] - 4s 881us/step - loss: 0.1454 - avg_multilabel_BA_2: 0.8284 - val_loss: 0.1465 - val_avg_multilabel_BA_2: 0.8295\n",
      "Epoch 8/100\n",
      "4020/4020 [==============================] - 4s 880us/step - loss: 0.1437 - avg_multilabel_BA_2: 0.8306 - val_loss: 0.1436 - val_avg_multilabel_BA_2: 0.8315\n",
      "Epoch 9/100\n",
      "4020/4020 [==============================] - 4s 880us/step - loss: 0.1424 - avg_multilabel_BA_2: 0.8325 - val_loss: 0.1413 - val_avg_multilabel_BA_2: 0.8334\n",
      "Epoch 10/100\n",
      "4020/4020 [==============================] - 4s 885us/step - loss: 0.1414 - avg_multilabel_BA_2: 0.8342 - val_loss: 0.1423 - val_avg_multilabel_BA_2: 0.8350\n",
      "Epoch 11/100\n",
      "4020/4020 [==============================] - 4s 890us/step - loss: 0.1405 - avg_multilabel_BA_2: 0.8357 - val_loss: 0.1403 - val_avg_multilabel_BA_2: 0.8364\n",
      "Epoch 12/100\n",
      "4020/4020 [==============================] - 4s 888us/step - loss: 0.1398 - avg_multilabel_BA_2: 0.8371 - val_loss: 0.1407 - val_avg_multilabel_BA_2: 0.8377\n",
      "Epoch 13/100\n",
      "4020/4020 [==============================] - 4s 882us/step - loss: 0.1390 - avg_multilabel_BA_2: 0.8384 - val_loss: 0.1382 - val_avg_multilabel_BA_2: 0.8389\n",
      "Epoch 14/100\n",
      "4020/4020 [==============================] - 4s 879us/step - loss: 0.1383 - avg_multilabel_BA_2: 0.8394 - val_loss: 0.1430 - val_avg_multilabel_BA_2: 0.8399\n",
      "Epoch 15/100\n",
      "4020/4020 [==============================] - 4s 880us/step - loss: 0.1377 - avg_multilabel_BA_2: 0.8404 - val_loss: 0.1382 - val_avg_multilabel_BA_2: 0.8409\n",
      "Epoch 16/100\n",
      "4020/4020 [==============================] - 4s 882us/step - loss: 0.1372 - avg_multilabel_BA_2: 0.8413 - val_loss: 0.1376 - val_avg_multilabel_BA_2: 0.8418\n",
      "Epoch 17/100\n",
      "4020/4020 [==============================] - 4s 879us/step - loss: 0.1368 - avg_multilabel_BA_2: 0.8422 - val_loss: 0.1362 - val_avg_multilabel_BA_2: 0.8426\n",
      "Epoch 18/100\n",
      "4020/4020 [==============================] - 4s 881us/step - loss: 0.1362 - avg_multilabel_BA_2: 0.8430 - val_loss: 0.1378 - val_avg_multilabel_BA_2: 0.8433\n",
      "Epoch 19/100\n",
      "4020/4020 [==============================] - 4s 883us/step - loss: 0.1359 - avg_multilabel_BA_2: 0.8436 - val_loss: 0.1372 - val_avg_multilabel_BA_2: 0.8439\n",
      "Epoch 20/100\n",
      "4020/4020 [==============================] - 4s 877us/step - loss: 0.1354 - avg_multilabel_BA_2: 0.8443 - val_loss: 0.1400 - val_avg_multilabel_BA_2: 0.8445\n",
      "Epoch 21/100\n",
      "4020/4020 [==============================] - 3s 860us/step - loss: 0.1350 - avg_multilabel_BA_2: 0.8448 - val_loss: 0.1351 - val_avg_multilabel_BA_2: 0.8451\n",
      "Epoch 22/100\n",
      "4020/4020 [==============================] - 3s 870us/step - loss: 0.1346 - avg_multilabel_BA_2: 0.8454 - val_loss: 0.1354 - val_avg_multilabel_BA_2: 0.8457\n",
      "Epoch 23/100\n",
      "4020/4020 [==============================] - 4s 879us/step - loss: 0.1342 - avg_multilabel_BA_2: 0.8459 - val_loss: 0.1354 - val_avg_multilabel_BA_2: 0.8462\n",
      "Epoch 24/100\n",
      "4020/4020 [==============================] - 4s 878us/step - loss: 0.1339 - avg_multilabel_BA_2: 0.8466 - val_loss: 0.1362 - val_avg_multilabel_BA_2: 0.8468\n",
      "Epoch 25/100\n",
      "4020/4020 [==============================] - 4s 878us/step - loss: 0.1336 - avg_multilabel_BA_2: 0.8471 - val_loss: 0.1336 - val_avg_multilabel_BA_2: 0.8473\n",
      "Epoch 26/100\n",
      "4020/4020 [==============================] - 4s 880us/step - loss: 0.1332 - avg_multilabel_BA_2: 0.8475 - val_loss: 0.1341 - val_avg_multilabel_BA_2: 0.8478\n",
      "Epoch 27/100\n",
      "4020/4020 [==============================] - 4s 876us/step - loss: 0.1328 - avg_multilabel_BA_2: 0.8480 - val_loss: 0.1343 - val_avg_multilabel_BA_2: 0.8482\n",
      "Epoch 28/100\n",
      "4020/4020 [==============================] - 4s 885us/step - loss: 0.1326 - avg_multilabel_BA_2: 0.8485 - val_loss: 0.1335 - val_avg_multilabel_BA_2: 0.8487\n",
      "Epoch 29/100\n",
      "4020/4020 [==============================] - 4s 875us/step - loss: 0.1323 - avg_multilabel_BA_2: 0.8489 - val_loss: 0.1372 - val_avg_multilabel_BA_2: 0.8491\n",
      "Epoch 30/100\n",
      "4020/4020 [==============================] - 4s 895us/step - loss: 0.1320 - avg_multilabel_BA_2: 0.8493 - val_loss: 0.1335 - val_avg_multilabel_BA_2: 0.8495\n",
      "Epoch 31/100\n",
      "4020/4020 [==============================] - 4s 882us/step - loss: 0.1318 - avg_multilabel_BA_2: 0.8497 - val_loss: 0.1331 - val_avg_multilabel_BA_2: 0.8499\n",
      "Epoch 32/100\n",
      "4020/4020 [==============================] - 4s 881us/step - loss: 0.1316 - avg_multilabel_BA_2: 0.8501 - val_loss: 0.1350 - val_avg_multilabel_BA_2: 0.8503\n",
      "Epoch 33/100\n",
      "4020/4020 [==============================] - 4s 899us/step - loss: 0.1313 - avg_multilabel_BA_2: 0.8504 - val_loss: 0.1328 - val_avg_multilabel_BA_2: 0.8506\n",
      "Epoch 34/100\n",
      "4020/4020 [==============================] - 4s 900us/step - loss: 0.1313 - avg_multilabel_BA_2: 0.8508 - val_loss: 0.1324 - val_avg_multilabel_BA_2: 0.8509\n",
      "Epoch 35/100\n",
      "4020/4020 [==============================] - 4s 883us/step - loss: 0.1309 - avg_multilabel_BA_2: 0.8511 - val_loss: 0.1317 - val_avg_multilabel_BA_2: 0.8513\n",
      "Epoch 36/100\n",
      "4020/4020 [==============================] - 4s 882us/step - loss: 0.1307 - avg_multilabel_BA_2: 0.8515 - val_loss: 0.1353 - val_avg_multilabel_BA_2: 0.8516\n",
      "Epoch 37/100\n",
      "4020/4020 [==============================] - 4s 879us/step - loss: 0.1305 - avg_multilabel_BA_2: 0.8517 - val_loss: 0.1336 - val_avg_multilabel_BA_2: 0.8519\n",
      "Epoch 38/100\n",
      "4020/4020 [==============================] - 4s 886us/step - loss: 0.1304 - avg_multilabel_BA_2: 0.8520 - val_loss: 0.1322 - val_avg_multilabel_BA_2: 0.8522\n",
      "Epoch 39/100\n",
      "4020/4020 [==============================] - 4s 880us/step - loss: 0.1302 - avg_multilabel_BA_2: 0.8524 - val_loss: 0.1348 - val_avg_multilabel_BA_2: 0.8525\n",
      "Epoch 40/100\n",
      "4020/4020 [==============================] - 4s 875us/step - loss: 0.1301 - avg_multilabel_BA_2: 0.8527 - val_loss: 0.1321 - val_avg_multilabel_BA_2: 0.8528\n",
      "Epoch 41/100\n",
      "4020/4020 [==============================] - 4s 882us/step - loss: 0.1298 - avg_multilabel_BA_2: 0.8529 - val_loss: 0.1336 - val_avg_multilabel_BA_2: 0.8530\n",
      "Epoch 42/100\n",
      "4020/4020 [==============================] - 4s 883us/step - loss: 0.1296 - avg_multilabel_BA_2: 0.8532 - val_loss: 0.1314 - val_avg_multilabel_BA_2: 0.8533\n",
      "Epoch 43/100\n",
      "4020/4020 [==============================] - 4s 877us/step - loss: 0.1294 - avg_multilabel_BA_2: 0.8534 - val_loss: 0.1313 - val_avg_multilabel_BA_2: 0.8535\n",
      "Epoch 44/100\n",
      "4020/4020 [==============================] - 4s 880us/step - loss: 0.1292 - avg_multilabel_BA_2: 0.8537 - val_loss: 0.1313 - val_avg_multilabel_BA_2: 0.8538\n",
      "Epoch 45/100\n",
      "4020/4020 [==============================] - 4s 871us/step - loss: 0.1291 - avg_multilabel_BA_2: 0.8539 - val_loss: 0.1310 - val_avg_multilabel_BA_2: 0.8540\n",
      "Epoch 46/100\n",
      "4020/4020 [==============================] - 3s 851us/step - loss: 0.1288 - avg_multilabel_BA_2: 0.8542 - val_loss: 0.1317 - val_avg_multilabel_BA_2: 0.8542\n",
      "Epoch 47/100\n",
      "4020/4020 [==============================] - 3s 855us/step - loss: 0.1287 - avg_multilabel_BA_2: 0.8544 - val_loss: 0.1313 - val_avg_multilabel_BA_2: 0.8545\n",
      "Epoch 48/100\n",
      "4020/4020 [==============================] - 4s 878us/step - loss: 0.1288 - avg_multilabel_BA_2: 0.8546 - val_loss: 0.1320 - val_avg_multilabel_BA_2: 0.8547\n",
      "Epoch 49/100\n",
      "4020/4020 [==============================] - 4s 880us/step - loss: 0.1285 - avg_multilabel_BA_2: 0.8547 - val_loss: 0.1345 - val_avg_multilabel_BA_2: 0.8549\n",
      "Epoch 50/100\n",
      "4020/4020 [==============================] - 4s 880us/step - loss: 0.1284 - avg_multilabel_BA_2: 0.8550 - val_loss: 0.1309 - val_avg_multilabel_BA_2: 0.8551\n",
      "Epoch 51/100\n",
      "4020/4020 [==============================] - 4s 879us/step - loss: 0.1282 - avg_multilabel_BA_2: 0.8552 - val_loss: 0.1300 - val_avg_multilabel_BA_2: 0.8553\n",
      "Epoch 52/100\n",
      "4020/4020 [==============================] - 4s 879us/step - loss: 0.1282 - avg_multilabel_BA_2: 0.8554 - val_loss: 0.1298 - val_avg_multilabel_BA_2: 0.8555\n",
      "Epoch 53/100\n",
      "4020/4020 [==============================] - 4s 879us/step - loss: 0.1280 - avg_multilabel_BA_2: 0.8556 - val_loss: 0.1305 - val_avg_multilabel_BA_2: 0.8557\n",
      "Epoch 54/100\n",
      "4020/4020 [==============================] - 4s 880us/step - loss: 0.1280 - avg_multilabel_BA_2: 0.8559 - val_loss: 0.1291 - val_avg_multilabel_BA_2: 0.8559\n",
      "Epoch 55/100\n",
      "4020/4020 [==============================] - 4s 884us/step - loss: 0.1277 - avg_multilabel_BA_2: 0.8560 - val_loss: 0.1317 - val_avg_multilabel_BA_2: 0.8561\n",
      "Epoch 56/100\n",
      "4020/4020 [==============================] - 4s 878us/step - loss: 0.1277 - avg_multilabel_BA_2: 0.8562 - val_loss: 0.1309 - val_avg_multilabel_BA_2: 0.8563\n",
      "Epoch 57/100\n",
      "4020/4020 [==============================] - 4s 880us/step - loss: 0.1274 - avg_multilabel_BA_2: 0.8564 - val_loss: 0.1314 - val_avg_multilabel_BA_2: 0.8564\n",
      "Epoch 58/100\n",
      "4020/4020 [==============================] - 4s 879us/step - loss: 0.1274 - avg_multilabel_BA_2: 0.8566 - val_loss: 0.1324 - val_avg_multilabel_BA_2: 0.8566\n",
      "Epoch 59/100\n",
      "4020/4020 [==============================] - 4s 878us/step - loss: 0.1273 - avg_multilabel_BA_2: 0.8567 - val_loss: 0.1313 - val_avg_multilabel_BA_2: 0.8568\n",
      "Epoch 60/100\n",
      "4020/4020 [==============================] - 4s 879us/step - loss: 0.1273 - avg_multilabel_BA_2: 0.8569 - val_loss: 0.1300 - val_avg_multilabel_BA_2: 0.8570\n",
      "Epoch 61/100\n",
      "4020/4020 [==============================] - 4s 879us/step - loss: 0.1271 - avg_multilabel_BA_2: 0.8571 - val_loss: 0.1328 - val_avg_multilabel_BA_2: 0.8571\n",
      "Epoch 62/100\n",
      "4020/4020 [==============================] - 4s 877us/step - loss: 0.1271 - avg_multilabel_BA_2: 0.8572 - val_loss: 0.1300 - val_avg_multilabel_BA_2: 0.8573\n",
      "Epoch 63/100\n",
      "4020/4020 [==============================] - 4s 881us/step - loss: 0.1269 - avg_multilabel_BA_2: 0.8573 - val_loss: 0.1302 - val_avg_multilabel_BA_2: 0.8574\n",
      "Epoch 64/100\n",
      "4020/4020 [==============================] - 4s 877us/step - loss: 0.1268 - avg_multilabel_BA_2: 0.8575 - val_loss: 0.1338 - val_avg_multilabel_BA_2: 0.8575\n",
      "Epoch 65/100\n",
      "4020/4020 [==============================] - 4s 881us/step - loss: 0.1267 - avg_multilabel_BA_2: 0.8576 - val_loss: 0.1315 - val_avg_multilabel_BA_2: 0.8577\n",
      "Epoch 66/100\n",
      "4020/4020 [==============================] - 4s 871us/step - loss: 0.1266 - avg_multilabel_BA_2: 0.8578 - val_loss: 0.1303 - val_avg_multilabel_BA_2: 0.8578\n",
      "Epoch 67/100\n",
      "4020/4020 [==============================] - 3s 854us/step - loss: 0.1265 - avg_multilabel_BA_2: 0.8579 - val_loss: 0.1332 - val_avg_multilabel_BA_2: 0.8580\n",
      "Epoch 68/100\n",
      "4020/4020 [==============================] - 4s 874us/step - loss: 0.1264 - avg_multilabel_BA_2: 0.8581 - val_loss: 0.1292 - val_avg_multilabel_BA_2: 0.8581\n",
      "Epoch 69/100\n",
      "4020/4020 [==============================] - 4s 878us/step - loss: 0.1264 - avg_multilabel_BA_2: 0.8581 - val_loss: 0.1285 - val_avg_multilabel_BA_2: 0.8583\n",
      "Epoch 70/100\n",
      "4020/4020 [==============================] - 4s 881us/step - loss: 0.1263 - avg_multilabel_BA_2: 0.8583 - val_loss: 0.1322 - val_avg_multilabel_BA_2: 0.8584\n",
      "Epoch 71/100\n",
      "4020/4020 [==============================] - 4s 875us/step - loss: 0.1261 - avg_multilabel_BA_2: 0.8584 - val_loss: 0.1282 - val_avg_multilabel_BA_2: 0.8585\n",
      "Epoch 72/100\n",
      "4020/4020 [==============================] - 4s 888us/step - loss: 0.1261 - avg_multilabel_BA_2: 0.8586 - val_loss: 0.1286 - val_avg_multilabel_BA_2: 0.8587\n",
      "Epoch 73/100\n",
      "4020/4020 [==============================] - 4s 880us/step - loss: 0.1260 - avg_multilabel_BA_2: 0.8588 - val_loss: 0.1290 - val_avg_multilabel_BA_2: 0.8588\n",
      "Epoch 74/100\n",
      "4020/4020 [==============================] - 4s 887us/step - loss: 0.1258 - avg_multilabel_BA_2: 0.8589 - val_loss: 0.1288 - val_avg_multilabel_BA_2: 0.8589\n",
      "Epoch 75/100\n",
      "4020/4020 [==============================] - 3s 864us/step - loss: 0.1258 - avg_multilabel_BA_2: 0.8590 - val_loss: 0.1320 - val_avg_multilabel_BA_2: 0.8591\n",
      "Epoch 76/100\n",
      "4020/4020 [==============================] - 3s 847us/step - loss: 0.1258 - avg_multilabel_BA_2: 0.8591 - val_loss: 0.1294 - val_avg_multilabel_BA_2: 0.8592\n",
      "Epoch 77/100\n",
      "4020/4020 [==============================] - 3s 844us/step - loss: 0.1257 - avg_multilabel_BA_2: 0.8593 - val_loss: 0.1293 - val_avg_multilabel_BA_2: 0.8593\n",
      "Epoch 78/100\n",
      "4020/4020 [==============================] - 3s 841us/step - loss: 0.1257 - avg_multilabel_BA_2: 0.8594 - val_loss: 0.1290 - val_avg_multilabel_BA_2: 0.8594\n",
      "Epoch 79/100\n",
      "4020/4020 [==============================] - 4s 873us/step - loss: 0.1255 - avg_multilabel_BA_2: 0.8595 - val_loss: 0.1287 - val_avg_multilabel_BA_2: 0.8595\n",
      "Epoch 80/100\n",
      "4020/4020 [==============================] - 4s 879us/step - loss: 0.1254 - avg_multilabel_BA_2: 0.8596 - val_loss: 0.1288 - val_avg_multilabel_BA_2: 0.8597\n",
      "Epoch 81/100\n",
      "4020/4020 [==============================] - 4s 879us/step - loss: 0.1254 - avg_multilabel_BA_2: 0.8597 - val_loss: 0.1309 - val_avg_multilabel_BA_2: 0.8598\n",
      "Epoch 82/100\n",
      "4020/4020 [==============================] - 4s 879us/step - loss: 0.1253 - avg_multilabel_BA_2: 0.8598 - val_loss: 0.1276 - val_avg_multilabel_BA_2: 0.8599\n",
      "Epoch 83/100\n",
      "4020/4020 [==============================] - 4s 887us/step - loss: 0.1253 - avg_multilabel_BA_2: 0.8599 - val_loss: 0.1294 - val_avg_multilabel_BA_2: 0.8600\n",
      "Epoch 84/100\n",
      "4020/4020 [==============================] - 4s 872us/step - loss: 0.1252 - avg_multilabel_BA_2: 0.8601 - val_loss: 0.1307 - val_avg_multilabel_BA_2: 0.8601\n",
      "Epoch 85/100\n",
      "4020/4020 [==============================] - 4s 880us/step - loss: 0.1252 - avg_multilabel_BA_2: 0.8601 - val_loss: 0.1273 - val_avg_multilabel_BA_2: 0.8602\n",
      "Epoch 86/100\n",
      "4020/4020 [==============================] - 4s 876us/step - loss: 0.1251 - avg_multilabel_BA_2: 0.8603 - val_loss: 0.1285 - val_avg_multilabel_BA_2: 0.8603\n",
      "Epoch 87/100\n",
      "4020/4020 [==============================] - 4s 876us/step - loss: 0.1250 - avg_multilabel_BA_2: 0.8604 - val_loss: 0.1275 - val_avg_multilabel_BA_2: 0.8604\n",
      "Epoch 88/100\n",
      "4020/4020 [==============================] - 4s 878us/step - loss: 0.1248 - avg_multilabel_BA_2: 0.8605 - val_loss: 0.1273 - val_avg_multilabel_BA_2: 0.8605\n",
      "Epoch 89/100\n",
      "4020/4020 [==============================] - 4s 881us/step - loss: 0.1249 - avg_multilabel_BA_2: 0.8606 - val_loss: 0.1309 - val_avg_multilabel_BA_2: 0.8606\n",
      "Epoch 90/100\n",
      "4020/4020 [==============================] - 4s 877us/step - loss: 0.1248 - avg_multilabel_BA_2: 0.8607 - val_loss: 0.1305 - val_avg_multilabel_BA_2: 0.8607\n",
      "Epoch 91/100\n",
      "4020/4020 [==============================] - 4s 879us/step - loss: 0.1248 - avg_multilabel_BA_2: 0.8608 - val_loss: 0.1272 - val_avg_multilabel_BA_2: 0.8608\n",
      "Epoch 92/100\n",
      "4020/4020 [==============================] - 4s 882us/step - loss: 0.1248 - avg_multilabel_BA_2: 0.8609 - val_loss: 0.1291 - val_avg_multilabel_BA_2: 0.8609\n",
      "Epoch 93/100\n",
      "4020/4020 [==============================] - 4s 881us/step - loss: 0.1245 - avg_multilabel_BA_2: 0.8610 - val_loss: 0.1289 - val_avg_multilabel_BA_2: 0.8610\n",
      "Epoch 94/100\n",
      "4020/4020 [==============================] - 4s 875us/step - loss: 0.1245 - avg_multilabel_BA_2: 0.8611 - val_loss: 0.1274 - val_avg_multilabel_BA_2: 0.8611\n",
      "Epoch 95/100\n",
      "4020/4020 [==============================] - 4s 872us/step - loss: 0.1245 - avg_multilabel_BA_2: 0.8612 - val_loss: 0.1273 - val_avg_multilabel_BA_2: 0.8612\n",
      "Epoch 96/100\n",
      "4020/4020 [==============================] - 4s 877us/step - loss: 0.1245 - avg_multilabel_BA_2: 0.8613 - val_loss: 0.1276 - val_avg_multilabel_BA_2: 0.8613\n",
      "Epoch 97/100\n",
      "4020/4020 [==============================] - 4s 882us/step - loss: 0.1245 - avg_multilabel_BA_2: 0.8613 - val_loss: 0.1268 - val_avg_multilabel_BA_2: 0.8614\n",
      "Epoch 98/100\n",
      "4020/4020 [==============================] - 4s 880us/step - loss: 0.1243 - avg_multilabel_BA_2: 0.8615 - val_loss: 0.1282 - val_avg_multilabel_BA_2: 0.8615\n",
      "Epoch 99/100\n",
      "4020/4020 [==============================] - 4s 878us/step - loss: 0.1243 - avg_multilabel_BA_2: 0.8616 - val_loss: 0.1283 - val_avg_multilabel_BA_2: 0.8616\n",
      "Epoch 100/100\n",
      "4020/4020 [==============================] - 4s 879us/step - loss: 0.1242 - avg_multilabel_BA_2: 0.8616 - val_loss: 0.1281 - val_avg_multilabel_BA_2: 0.8617\n",
      "Best epoch: 100\n",
      "Epoch 1/100\n",
      "4020/4020 [==============================] - 4s 969us/step - loss: 0.1957 - avg_multilabel_BA_2: 0.8613 - val_loss: 0.1724 - val_avg_multilabel_BA_2: 0.8610\n",
      "Epoch 2/100\n",
      "4020/4020 [==============================] - 4s 880us/step - loss: 0.1624 - avg_multilabel_BA_2: 0.8608 - val_loss: 0.1557 - val_avg_multilabel_BA_2: 0.8607\n",
      "Epoch 3/100\n",
      "4020/4020 [==============================] - 4s 884us/step - loss: 0.1553 - avg_multilabel_BA_2: 0.8605 - val_loss: 0.1520 - val_avg_multilabel_BA_2: 0.8604\n",
      "Epoch 4/100\n",
      "4020/4020 [==============================] - 4s 877us/step - loss: 0.1515 - avg_multilabel_BA_2: 0.8603 - val_loss: 0.1511 - val_avg_multilabel_BA_2: 0.8602\n",
      "Epoch 5/100\n",
      "4020/4020 [==============================] - 4s 880us/step - loss: 0.1485 - avg_multilabel_BA_2: 0.8601 - val_loss: 0.1470 - val_avg_multilabel_BA_2: 0.8600\n",
      "Epoch 6/100\n",
      "4020/4020 [==============================] - 4s 885us/step - loss: 0.1464 - avg_multilabel_BA_2: 0.8599 - val_loss: 0.1454 - val_avg_multilabel_BA_2: 0.8598\n",
      "Epoch 7/100\n",
      "4020/4020 [==============================] - 4s 878us/step - loss: 0.1448 - avg_multilabel_BA_2: 0.8597 - val_loss: 0.1448 - val_avg_multilabel_BA_2: 0.8597\n",
      "Epoch 8/100\n",
      "4020/4020 [==============================] - 4s 884us/step - loss: 0.1432 - avg_multilabel_BA_2: 0.8596 - val_loss: 0.1434 - val_avg_multilabel_BA_2: 0.8596\n",
      "Epoch 9/100\n",
      "4020/4020 [==============================] - 4s 884us/step - loss: 0.1421 - avg_multilabel_BA_2: 0.8595 - val_loss: 0.1415 - val_avg_multilabel_BA_2: 0.8595\n",
      "Epoch 10/100\n",
      "4020/4020 [==============================] - 4s 886us/step - loss: 0.1409 - avg_multilabel_BA_2: 0.8594 - val_loss: 0.1401 - val_avg_multilabel_BA_2: 0.8594\n",
      "Epoch 11/100\n",
      "4020/4020 [==============================] - 4s 880us/step - loss: 0.1398 - avg_multilabel_BA_2: 0.8594 - val_loss: 0.1390 - val_avg_multilabel_BA_2: 0.8593\n",
      "Epoch 12/100\n",
      "4020/4020 [==============================] - 4s 883us/step - loss: 0.1390 - avg_multilabel_BA_2: 0.8593 - val_loss: 0.1389 - val_avg_multilabel_BA_2: 0.8593\n",
      "Epoch 13/100\n",
      "4020/4020 [==============================] - 4s 876us/step - loss: 0.1381 - avg_multilabel_BA_2: 0.8592 - val_loss: 0.1383 - val_avg_multilabel_BA_2: 0.8592\n",
      "Epoch 14/100\n",
      "4020/4020 [==============================] - 4s 887us/step - loss: 0.1374 - avg_multilabel_BA_2: 0.8591 - val_loss: 0.1432 - val_avg_multilabel_BA_2: 0.8591\n",
      "Epoch 15/100\n",
      "4020/4020 [==============================] - 4s 880us/step - loss: 0.1369 - avg_multilabel_BA_2: 0.8591 - val_loss: 0.1387 - val_avg_multilabel_BA_2: 0.8591\n",
      "Epoch 16/100\n",
      "4020/4020 [==============================] - 4s 885us/step - loss: 0.1361 - avg_multilabel_BA_2: 0.8591 - val_loss: 0.1363 - val_avg_multilabel_BA_2: 0.8591\n",
      "Epoch 17/100\n",
      "4020/4020 [==============================] - 4s 884us/step - loss: 0.1355 - avg_multilabel_BA_2: 0.8591 - val_loss: 0.1381 - val_avg_multilabel_BA_2: 0.8590\n",
      "Epoch 18/100\n",
      "4020/4020 [==============================] - 4s 879us/step - loss: 0.1351 - avg_multilabel_BA_2: 0.8591 - val_loss: 0.1358 - val_avg_multilabel_BA_2: 0.8590\n",
      "Epoch 19/100\n",
      "4020/4020 [==============================] - 4s 885us/step - loss: 0.1348 - avg_multilabel_BA_2: 0.8590 - val_loss: 0.1356 - val_avg_multilabel_BA_2: 0.8590\n",
      "Epoch 20/100\n",
      "4020/4020 [==============================] - 4s 886us/step - loss: 0.1342 - avg_multilabel_BA_2: 0.8590 - val_loss: 0.1350 - val_avg_multilabel_BA_2: 0.8590\n",
      "Epoch 21/100\n",
      "4020/4020 [==============================] - 4s 881us/step - loss: 0.1339 - avg_multilabel_BA_2: 0.8590 - val_loss: 0.1340 - val_avg_multilabel_BA_2: 0.8590\n",
      "Epoch 22/100\n",
      "4020/4020 [==============================] - 4s 896us/step - loss: 0.1334 - avg_multilabel_BA_2: 0.8590 - val_loss: 0.1357 - val_avg_multilabel_BA_2: 0.8590\n",
      "Epoch 23/100\n",
      "4020/4020 [==============================] - 3s 860us/step - loss: 0.1330 - avg_multilabel_BA_2: 0.8590 - val_loss: 0.1344 - val_avg_multilabel_BA_2: 0.8590\n",
      "Epoch 24/100\n",
      "4020/4020 [==============================] - 3s 861us/step - loss: 0.1325 - avg_multilabel_BA_2: 0.8590 - val_loss: 0.1344 - val_avg_multilabel_BA_2: 0.8590\n",
      "Epoch 25/100\n",
      "4020/4020 [==============================] - 3s 861us/step - loss: 0.1324 - avg_multilabel_BA_2: 0.8590 - val_loss: 0.1353 - val_avg_multilabel_BA_2: 0.8590\n",
      "Epoch 26/100\n",
      "4020/4020 [==============================] - 3s 859us/step - loss: 0.1320 - avg_multilabel_BA_2: 0.8591 - val_loss: 0.1354 - val_avg_multilabel_BA_2: 0.8590\n",
      "Epoch 27/100\n",
      "4020/4020 [==============================] - 3s 857us/step - loss: 0.1317 - avg_multilabel_BA_2: 0.8591 - val_loss: 0.1335 - val_avg_multilabel_BA_2: 0.8590\n",
      "Epoch 28/100\n",
      "4020/4020 [==============================] - 3s 859us/step - loss: 0.1314 - avg_multilabel_BA_2: 0.8591 - val_loss: 0.1342 - val_avg_multilabel_BA_2: 0.8591\n",
      "Epoch 29/100\n",
      "4020/4020 [==============================] - 3s 859us/step - loss: 0.1311 - avg_multilabel_BA_2: 0.8591 - val_loss: 0.1336 - val_avg_multilabel_BA_2: 0.8591\n",
      "Epoch 30/100\n",
      "4020/4020 [==============================] - 3s 861us/step - loss: 0.1308 - avg_multilabel_BA_2: 0.8591 - val_loss: 0.1333 - val_avg_multilabel_BA_2: 0.8591\n",
      "Epoch 31/100\n",
      "4020/4020 [==============================] - 3s 854us/step - loss: 0.1305 - avg_multilabel_BA_2: 0.8591 - val_loss: 0.1332 - val_avg_multilabel_BA_2: 0.8591\n",
      "Epoch 32/100\n",
      "4020/4020 [==============================] - 3s 861us/step - loss: 0.1303 - avg_multilabel_BA_2: 0.8591 - val_loss: 0.1323 - val_avg_multilabel_BA_2: 0.8591\n",
      "Epoch 33/100\n",
      "4020/4020 [==============================] - 3s 861us/step - loss: 0.1300 - avg_multilabel_BA_2: 0.8591 - val_loss: 0.1326 - val_avg_multilabel_BA_2: 0.8592\n",
      "Epoch 34/100\n",
      "4020/4020 [==============================] - 3s 860us/step - loss: 0.1297 - avg_multilabel_BA_2: 0.8591 - val_loss: 0.1319 - val_avg_multilabel_BA_2: 0.8592\n",
      "Epoch 35/100\n",
      "4020/4020 [==============================] - 3s 863us/step - loss: 0.1296 - avg_multilabel_BA_2: 0.8592 - val_loss: 0.1326 - val_avg_multilabel_BA_2: 0.8592\n",
      "Epoch 36/100\n",
      "4020/4020 [==============================] - 3s 863us/step - loss: 0.1294 - avg_multilabel_BA_2: 0.8593 - val_loss: 0.1319 - val_avg_multilabel_BA_2: 0.8593\n",
      "Epoch 37/100\n",
      "4020/4020 [==============================] - 3s 858us/step - loss: 0.1291 - avg_multilabel_BA_2: 0.8593 - val_loss: 0.1313 - val_avg_multilabel_BA_2: 0.8593\n",
      "Epoch 38/100\n",
      "4020/4020 [==============================] - 3s 861us/step - loss: 0.1289 - avg_multilabel_BA_2: 0.8594 - val_loss: 0.1302 - val_avg_multilabel_BA_2: 0.8594\n",
      "Epoch 39/100\n",
      "4020/4020 [==============================] - 3s 864us/step - loss: 0.1288 - avg_multilabel_BA_2: 0.8594 - val_loss: 0.1327 - val_avg_multilabel_BA_2: 0.8594\n",
      "Epoch 40/100\n",
      "4020/4020 [==============================] - 3s 860us/step - loss: 0.1286 - avg_multilabel_BA_2: 0.8594 - val_loss: 0.1321 - val_avg_multilabel_BA_2: 0.8594\n",
      "Epoch 41/100\n",
      "4020/4020 [==============================] - 3s 857us/step - loss: 0.1283 - avg_multilabel_BA_2: 0.8594 - val_loss: 0.1315 - val_avg_multilabel_BA_2: 0.8595\n",
      "Epoch 42/100\n",
      "4020/4020 [==============================] - 3s 857us/step - loss: 0.1281 - avg_multilabel_BA_2: 0.8595 - val_loss: 0.1295 - val_avg_multilabel_BA_2: 0.8595\n",
      "Epoch 43/100\n",
      "4020/4020 [==============================] - 3s 858us/step - loss: 0.1279 - avg_multilabel_BA_2: 0.8596 - val_loss: 0.1304 - val_avg_multilabel_BA_2: 0.8596\n",
      "Epoch 44/100\n",
      "4020/4020 [==============================] - 3s 862us/step - loss: 0.1278 - avg_multilabel_BA_2: 0.8596 - val_loss: 0.1315 - val_avg_multilabel_BA_2: 0.8596\n",
      "Epoch 45/100\n",
      "4020/4020 [==============================] - 3s 861us/step - loss: 0.1277 - avg_multilabel_BA_2: 0.8596 - val_loss: 0.1305 - val_avg_multilabel_BA_2: 0.8597\n",
      "Epoch 46/100\n",
      "4020/4020 [==============================] - 3s 859us/step - loss: 0.1275 - avg_multilabel_BA_2: 0.8597 - val_loss: 0.1324 - val_avg_multilabel_BA_2: 0.8597\n",
      "Epoch 47/100\n",
      "4020/4020 [==============================] - 3s 857us/step - loss: 0.1275 - avg_multilabel_BA_2: 0.8597 - val_loss: 0.1309 - val_avg_multilabel_BA_2: 0.8597\n",
      "Epoch 48/100\n",
      "4020/4020 [==============================] - 3s 854us/step - loss: 0.1271 - avg_multilabel_BA_2: 0.8598 - val_loss: 0.1308 - val_avg_multilabel_BA_2: 0.8598\n",
      "Epoch 49/100\n",
      "4020/4020 [==============================] - 3s 862us/step - loss: 0.1271 - avg_multilabel_BA_2: 0.8598 - val_loss: 0.1297 - val_avg_multilabel_BA_2: 0.8598\n",
      "Epoch 50/100\n",
      "4020/4020 [==============================] - 3s 859us/step - loss: 0.1270 - avg_multilabel_BA_2: 0.8599 - val_loss: 0.1308 - val_avg_multilabel_BA_2: 0.8599\n",
      "Epoch 51/100\n",
      "4020/4020 [==============================] - 3s 868us/step - loss: 0.1267 - avg_multilabel_BA_2: 0.8599 - val_loss: 0.1301 - val_avg_multilabel_BA_2: 0.8599\n",
      "Epoch 52/100\n",
      "4020/4020 [==============================] - 4s 874us/step - loss: 0.1266 - avg_multilabel_BA_2: 0.8599 - val_loss: 0.1293 - val_avg_multilabel_BA_2: 0.8600\n",
      "Epoch 53/100\n",
      "4020/4020 [==============================] - 4s 900us/step - loss: 0.1266 - avg_multilabel_BA_2: 0.8600 - val_loss: 0.1289 - val_avg_multilabel_BA_2: 0.8600\n",
      "Epoch 54/100\n",
      "4020/4020 [==============================] - 4s 901us/step - loss: 0.1263 - avg_multilabel_BA_2: 0.8601 - val_loss: 0.1327 - val_avg_multilabel_BA_2: 0.8601\n",
      "Epoch 55/100\n",
      "4020/4020 [==============================] - 4s 934us/step - loss: 0.1263 - avg_multilabel_BA_2: 0.8601 - val_loss: 0.1291 - val_avg_multilabel_BA_2: 0.8601\n",
      "Epoch 56/100\n",
      "4020/4020 [==============================] - 4s 901us/step - loss: 0.1261 - avg_multilabel_BA_2: 0.8601 - val_loss: 0.1317 - val_avg_multilabel_BA_2: 0.8602\n",
      "Epoch 57/100\n",
      "4020/4020 [==============================] - 4s 1ms/step - loss: 0.1260 - avg_multilabel_BA_2: 0.8602 - val_loss: 0.1300 - val_avg_multilabel_BA_2: 0.8602\n",
      "Epoch 58/100\n",
      "4020/4020 [==============================] - 4s 925us/step - loss: 0.1258 - avg_multilabel_BA_2: 0.8603 - val_loss: 0.1300 - val_avg_multilabel_BA_2: 0.8603\n",
      "Epoch 59/100\n",
      "4020/4020 [==============================] - 4s 926us/step - loss: 0.1257 - avg_multilabel_BA_2: 0.8603 - val_loss: 0.1293 - val_avg_multilabel_BA_2: 0.8603\n",
      "Epoch 60/100\n",
      "4020/4020 [==============================] - 4s 916us/step - loss: 0.1256 - avg_multilabel_BA_2: 0.8604 - val_loss: 0.1288 - val_avg_multilabel_BA_2: 0.8604\n",
      "Epoch 61/100\n",
      "4020/4020 [==============================] - 4s 893us/step - loss: 0.1254 - avg_multilabel_BA_2: 0.8604 - val_loss: 0.1302 - val_avg_multilabel_BA_2: 0.8604\n",
      "Epoch 62/100\n",
      "4020/4020 [==============================] - 4s 902us/step - loss: 0.1254 - avg_multilabel_BA_2: 0.8605 - val_loss: 0.1300 - val_avg_multilabel_BA_2: 0.8605\n",
      "Epoch 63/100\n",
      "4020/4020 [==============================] - 4s 909us/step - loss: 0.1253 - avg_multilabel_BA_2: 0.8605 - val_loss: 0.1325 - val_avg_multilabel_BA_2: 0.8605\n",
      "Epoch 64/100\n",
      "4020/4020 [==============================] - 4s 920us/step - loss: 0.1252 - avg_multilabel_BA_2: 0.8606 - val_loss: 0.1280 - val_avg_multilabel_BA_2: 0.8606\n",
      "Epoch 65/100\n",
      "4020/4020 [==============================] - 4s 897us/step - loss: 0.1251 - avg_multilabel_BA_2: 0.8606 - val_loss: 0.1300 - val_avg_multilabel_BA_2: 0.8607\n",
      "Epoch 66/100\n",
      "4020/4020 [==============================] - 4s 904us/step - loss: 0.1250 - avg_multilabel_BA_2: 0.8607 - val_loss: 0.1315 - val_avg_multilabel_BA_2: 0.8607\n",
      "Epoch 67/100\n",
      "4020/4020 [==============================] - 4s 909us/step - loss: 0.1249 - avg_multilabel_BA_2: 0.8608 - val_loss: 0.1291 - val_avg_multilabel_BA_2: 0.8608\n",
      "Epoch 68/100\n",
      "4020/4020 [==============================] - 4s 898us/step - loss: 0.1247 - avg_multilabel_BA_2: 0.8608 - val_loss: 0.1286 - val_avg_multilabel_BA_2: 0.8608\n",
      "Epoch 69/100\n",
      "4020/4020 [==============================] - 4s 922us/step - loss: 0.1247 - avg_multilabel_BA_2: 0.8608 - val_loss: 0.1304 - val_avg_multilabel_BA_2: 0.8609\n",
      "Epoch 70/100\n",
      "4020/4020 [==============================] - 4s 910us/step - loss: 0.1244 - avg_multilabel_BA_2: 0.8609 - val_loss: 0.1312 - val_avg_multilabel_BA_2: 0.8609\n",
      "Epoch 71/100\n",
      "4020/4020 [==============================] - 4s 896us/step - loss: 0.1244 - avg_multilabel_BA_2: 0.8610 - val_loss: 0.1283 - val_avg_multilabel_BA_2: 0.8610\n",
      "Epoch 72/100\n",
      "4020/4020 [==============================] - 4s 909us/step - loss: 0.1244 - avg_multilabel_BA_2: 0.8610 - val_loss: 0.1299 - val_avg_multilabel_BA_2: 0.8611\n",
      "Epoch 73/100\n",
      "4020/4020 [==============================] - 4s 918us/step - loss: 0.1241 - avg_multilabel_BA_2: 0.8611 - val_loss: 0.1283 - val_avg_multilabel_BA_2: 0.8611\n",
      "Epoch 74/100\n",
      "4020/4020 [==============================] - 4s 926us/step - loss: 0.1241 - avg_multilabel_BA_2: 0.8611 - val_loss: 0.1281 - val_avg_multilabel_BA_2: 0.8612\n",
      "Epoch 75/100\n",
      "4020/4020 [==============================] - 4s 910us/step - loss: 0.1241 - avg_multilabel_BA_2: 0.8612 - val_loss: 0.1285 - val_avg_multilabel_BA_2: 0.8612\n",
      "Epoch 76/100\n",
      "4020/4020 [==============================] - 4s 891us/step - loss: 0.1240 - avg_multilabel_BA_2: 0.8613 - val_loss: 0.1276 - val_avg_multilabel_BA_2: 0.8613\n",
      "Epoch 77/100\n",
      "4020/4020 [==============================] - 4s 886us/step - loss: 0.1238 - avg_multilabel_BA_2: 0.8613 - val_loss: 0.1286 - val_avg_multilabel_BA_2: 0.8613\n",
      "Epoch 78/100\n",
      "4020/4020 [==============================] - 3s 853us/step - loss: 0.1238 - avg_multilabel_BA_2: 0.8613 - val_loss: 0.1316 - val_avg_multilabel_BA_2: 0.8614\n",
      "Epoch 79/100\n",
      "4020/4020 [==============================] - 3s 862us/step - loss: 0.1238 - avg_multilabel_BA_2: 0.8614 - val_loss: 0.1309 - val_avg_multilabel_BA_2: 0.8614\n",
      "Epoch 80/100\n",
      "4020/4020 [==============================] - 4s 898us/step - loss: 0.1236 - avg_multilabel_BA_2: 0.8615 - val_loss: 0.1271 - val_avg_multilabel_BA_2: 0.8615\n",
      "Epoch 81/100\n",
      "4020/4020 [==============================] - 4s 931us/step - loss: 0.1235 - avg_multilabel_BA_2: 0.8615 - val_loss: 0.1273 - val_avg_multilabel_BA_2: 0.8615\n",
      "Epoch 82/100\n",
      "4020/4020 [==============================] - 4s 922us/step - loss: 0.1234 - avg_multilabel_BA_2: 0.8616 - val_loss: 0.1289 - val_avg_multilabel_BA_2: 0.8616\n",
      "Epoch 83/100\n",
      "4020/4020 [==============================] - 4s 890us/step - loss: 0.1233 - avg_multilabel_BA_2: 0.8616 - val_loss: 0.1276 - val_avg_multilabel_BA_2: 0.8616\n",
      "Epoch 84/100\n",
      "4020/4020 [==============================] - 4s 899us/step - loss: 0.1232 - avg_multilabel_BA_2: 0.8616 - val_loss: 0.1282 - val_avg_multilabel_BA_2: 0.8617\n",
      "Epoch 85/100\n",
      "4020/4020 [==============================] - 4s 884us/step - loss: 0.1233 - avg_multilabel_BA_2: 0.8617 - val_loss: 0.1284 - val_avg_multilabel_BA_2: 0.8618\n",
      "Epoch 86/100\n",
      "4020/4020 [==============================] - 4s 876us/step - loss: 0.1231 - avg_multilabel_BA_2: 0.8618 - val_loss: 0.1266 - val_avg_multilabel_BA_2: 0.8618\n",
      "Epoch 87/100\n",
      "4020/4020 [==============================] - 4s 891us/step - loss: 0.1230 - avg_multilabel_BA_2: 0.8618 - val_loss: 0.1267 - val_avg_multilabel_BA_2: 0.8619\n",
      "Epoch 88/100\n",
      "4020/4020 [==============================] - 4s 885us/step - loss: 0.1230 - avg_multilabel_BA_2: 0.8619 - val_loss: 0.1266 - val_avg_multilabel_BA_2: 0.8619\n",
      "Epoch 89/100\n",
      "4020/4020 [==============================] - 4s 873us/step - loss: 0.1229 - avg_multilabel_BA_2: 0.8620 - val_loss: 0.1268 - val_avg_multilabel_BA_2: 0.8620\n",
      "Epoch 90/100\n",
      "4020/4020 [==============================] - 4s 897us/step - loss: 0.1230 - avg_multilabel_BA_2: 0.8620 - val_loss: 0.1263 - val_avg_multilabel_BA_2: 0.8620\n",
      "Epoch 91/100\n",
      "4020/4020 [==============================] - 3s 861us/step - loss: 0.1230 - avg_multilabel_BA_2: 0.8621 - val_loss: 0.1297 - val_avg_multilabel_BA_2: 0.8621\n",
      "Epoch 92/100\n",
      "4020/4020 [==============================] - 3s 860us/step - loss: 0.1227 - avg_multilabel_BA_2: 0.8621 - val_loss: 0.1282 - val_avg_multilabel_BA_2: 0.8621\n",
      "Epoch 93/100\n",
      "4020/4020 [==============================] - 3s 869us/step - loss: 0.1225 - avg_multilabel_BA_2: 0.8621 - val_loss: 0.1273 - val_avg_multilabel_BA_2: 0.8622\n",
      "Epoch 94/100\n",
      "4020/4020 [==============================] - 4s 872us/step - loss: 0.1226 - avg_multilabel_BA_2: 0.8622 - val_loss: 0.1271 - val_avg_multilabel_BA_2: 0.8622\n",
      "Epoch 95/100\n",
      "4020/4020 [==============================] - 4s 884us/step - loss: 0.1226 - avg_multilabel_BA_2: 0.8623 - val_loss: 0.1277 - val_avg_multilabel_BA_2: 0.8623\n",
      "Epoch 96/100\n",
      "4020/4020 [==============================] - 4s 882us/step - loss: 0.1226 - avg_multilabel_BA_2: 0.8623 - val_loss: 0.1285 - val_avg_multilabel_BA_2: 0.8623\n",
      "Epoch 97/100\n",
      "4020/4020 [==============================] - 4s 886us/step - loss: 0.1225 - avg_multilabel_BA_2: 0.8623 - val_loss: 0.1266 - val_avg_multilabel_BA_2: 0.8624\n",
      "Epoch 98/100\n",
      "4020/4020 [==============================] - 4s 900us/step - loss: 0.1223 - avg_multilabel_BA_2: 0.8624 - val_loss: 0.1255 - val_avg_multilabel_BA_2: 0.8624\n",
      "Epoch 99/100\n",
      "4020/4020 [==============================] - 3s 864us/step - loss: 0.1223 - avg_multilabel_BA_2: 0.8625 - val_loss: 0.1279 - val_avg_multilabel_BA_2: 0.8625\n",
      "Epoch 100/100\n",
      "4020/4020 [==============================] - 3s 866us/step - loss: 0.1223 - avg_multilabel_BA_2: 0.8625 - val_loss: 0.1284 - val_avg_multilabel_BA_2: 0.8625\n",
      "1963/1963 [==============================] - 1s 548us/step - loss: 0.1309 - avg_multilabel_BA_2: 0.8625\n",
      "Test results - Loss: 0.13086728751659393 - Averaged Balanced Accuracy: 0.8625465035438538%\n",
      "Averaged Balanced Accuracy: 0.862556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-11 16:23:54.850044: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/saved_model_fold_0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-11 16:23:55.182829: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2022-04-11 16:23:55.182848: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "2022-04-11 16:23:55.182852: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:372] Ignored change_concat_input_ranges.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
      "2022-04-11 16:23:55.183479: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: model/saved_model_fold_0\n",
      "2022-04-11 16:23:55.184487: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-04-11 16:23:55.184499: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: model/saved_model_fold_0\n",
      "2022-04-11 16:23:55.186853: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-04-11 16:23:55.217143: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: model/saved_model_fold_0\n",
      "2022-04-11 16:23:55.225961: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 42485 microseconds.\n",
      "2022-04-11 16:23:55.244749: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project hypertunning/experimento_1_RandSearchCV/oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project hypertunning/experimento_1_RandSearchCV/oracle.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from hypertunning/experimento_1_RandSearchCV/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from hypertunning/experimento_1_RandSearchCV/tuner0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3806/3806 [==============================] - 4s 979us/step - loss: 0.1972 - avg_multilabel_BA_2: 0.8624 - val_loss: 0.1690 - val_avg_multilabel_BA_2: 0.8622\n",
      "Epoch 2/100\n",
      "3806/3806 [==============================] - 3s 885us/step - loss: 0.1600 - avg_multilabel_BA_2: 0.8621 - val_loss: 0.1584 - val_avg_multilabel_BA_2: 0.8621\n",
      "Epoch 3/100\n",
      "3806/3806 [==============================] - 3s 907us/step - loss: 0.1507 - avg_multilabel_BA_2: 0.8621 - val_loss: 0.1509 - val_avg_multilabel_BA_2: 0.8620\n",
      "Epoch 4/100\n",
      "3806/3806 [==============================] - 3s 911us/step - loss: 0.1461 - avg_multilabel_BA_2: 0.8619 - val_loss: 0.1466 - val_avg_multilabel_BA_2: 0.8619\n",
      "Epoch 5/100\n",
      "3806/3806 [==============================] - 3s 878us/step - loss: 0.1429 - avg_multilabel_BA_2: 0.8619 - val_loss: 0.1450 - val_avg_multilabel_BA_2: 0.8618\n",
      "Epoch 6/100\n",
      "3806/3806 [==============================] - 3s 876us/step - loss: 0.1405 - avg_multilabel_BA_2: 0.8618 - val_loss: 0.1400 - val_avg_multilabel_BA_2: 0.8618\n",
      "Epoch 7/100\n",
      "3806/3806 [==============================] - 3s 887us/step - loss: 0.1386 - avg_multilabel_BA_2: 0.8618 - val_loss: 0.1396 - val_avg_multilabel_BA_2: 0.8618\n",
      "Epoch 8/100\n",
      "3806/3806 [==============================] - 3s 858us/step - loss: 0.1370 - avg_multilabel_BA_2: 0.8618 - val_loss: 0.1375 - val_avg_multilabel_BA_2: 0.8617\n",
      "Epoch 9/100\n",
      "3806/3806 [==============================] - 3s 872us/step - loss: 0.1357 - avg_multilabel_BA_2: 0.8617 - val_loss: 0.1417 - val_avg_multilabel_BA_2: 0.8617\n",
      "Epoch 10/100\n",
      "3806/3806 [==============================] - 3s 913us/step - loss: 0.1344 - avg_multilabel_BA_2: 0.8617 - val_loss: 0.1348 - val_avg_multilabel_BA_2: 0.8617\n",
      "Epoch 11/100\n",
      "3806/3806 [==============================] - 3s 883us/step - loss: 0.1334 - avg_multilabel_BA_2: 0.8616 - val_loss: 0.1360 - val_avg_multilabel_BA_2: 0.8617\n",
      "Epoch 12/100\n",
      "3806/3806 [==============================] - 3s 873us/step - loss: 0.1325 - avg_multilabel_BA_2: 0.8616 - val_loss: 0.1338 - val_avg_multilabel_BA_2: 0.8617\n",
      "Epoch 13/100\n",
      "3806/3806 [==============================] - 3s 892us/step - loss: 0.1318 - avg_multilabel_BA_2: 0.8616 - val_loss: 0.1345 - val_avg_multilabel_BA_2: 0.8617\n",
      "Epoch 14/100\n",
      "3806/3806 [==============================] - 3s 883us/step - loss: 0.1310 - avg_multilabel_BA_2: 0.8617 - val_loss: 0.1318 - val_avg_multilabel_BA_2: 0.8617\n",
      "Epoch 15/100\n",
      "3806/3806 [==============================] - 3s 911us/step - loss: 0.1303 - avg_multilabel_BA_2: 0.8617 - val_loss: 0.1316 - val_avg_multilabel_BA_2: 0.8617\n",
      "Epoch 16/100\n",
      "3806/3806 [==============================] - 4s 923us/step - loss: 0.1297 - avg_multilabel_BA_2: 0.8617 - val_loss: 0.1321 - val_avg_multilabel_BA_2: 0.8617\n",
      "Epoch 17/100\n",
      "3806/3806 [==============================] - 3s 884us/step - loss: 0.1292 - avg_multilabel_BA_2: 0.8617 - val_loss: 0.1314 - val_avg_multilabel_BA_2: 0.8617\n",
      "Epoch 18/100\n",
      "3806/3806 [==============================] - 3s 910us/step - loss: 0.1286 - avg_multilabel_BA_2: 0.8618 - val_loss: 0.1300 - val_avg_multilabel_BA_2: 0.8618\n",
      "Epoch 19/100\n",
      "3806/3806 [==============================] - 3s 863us/step - loss: 0.1282 - avg_multilabel_BA_2: 0.8618 - val_loss: 0.1298 - val_avg_multilabel_BA_2: 0.8618\n",
      "Epoch 20/100\n",
      "3806/3806 [==============================] - 3s 865us/step - loss: 0.1278 - avg_multilabel_BA_2: 0.8618 - val_loss: 0.1306 - val_avg_multilabel_BA_2: 0.8618\n",
      "Epoch 21/100\n",
      "3806/3806 [==============================] - 3s 861us/step - loss: 0.1274 - avg_multilabel_BA_2: 0.8618 - val_loss: 0.1301 - val_avg_multilabel_BA_2: 0.8618\n",
      "Epoch 22/100\n",
      "3806/3806 [==============================] - 3s 843us/step - loss: 0.1269 - avg_multilabel_BA_2: 0.8618 - val_loss: 0.1297 - val_avg_multilabel_BA_2: 0.8618\n",
      "Epoch 23/100\n",
      "3806/3806 [==============================] - 3s 853us/step - loss: 0.1265 - avg_multilabel_BA_2: 0.8618 - val_loss: 0.1293 - val_avg_multilabel_BA_2: 0.8619\n",
      "Epoch 24/100\n",
      "3806/3806 [==============================] - 3s 863us/step - loss: 0.1261 - avg_multilabel_BA_2: 0.8618 - val_loss: 0.1278 - val_avg_multilabel_BA_2: 0.8619\n",
      "Epoch 25/100\n",
      "3806/3806 [==============================] - 3s 869us/step - loss: 0.1259 - avg_multilabel_BA_2: 0.8619 - val_loss: 0.1284 - val_avg_multilabel_BA_2: 0.8619\n",
      "Epoch 26/100\n",
      "3806/3806 [==============================] - 3s 884us/step - loss: 0.1256 - avg_multilabel_BA_2: 0.8619 - val_loss: 0.1274 - val_avg_multilabel_BA_2: 0.8619\n",
      "Epoch 27/100\n",
      "3806/3806 [==============================] - 3s 866us/step - loss: 0.1254 - avg_multilabel_BA_2: 0.8620 - val_loss: 0.1300 - val_avg_multilabel_BA_2: 0.8620\n",
      "Epoch 28/100\n",
      "3806/3806 [==============================] - 3s 853us/step - loss: 0.1251 - avg_multilabel_BA_2: 0.8620 - val_loss: 0.1269 - val_avg_multilabel_BA_2: 0.8620\n",
      "Epoch 29/100\n",
      "3806/3806 [==============================] - 3s 876us/step - loss: 0.1250 - avg_multilabel_BA_2: 0.8620 - val_loss: 0.1291 - val_avg_multilabel_BA_2: 0.8621\n",
      "Epoch 30/100\n",
      "3806/3806 [==============================] - 3s 855us/step - loss: 0.1246 - avg_multilabel_BA_2: 0.8621 - val_loss: 0.1271 - val_avg_multilabel_BA_2: 0.8621\n",
      "Epoch 31/100\n",
      "3806/3806 [==============================] - 3s 886us/step - loss: 0.1243 - avg_multilabel_BA_2: 0.8621 - val_loss: 0.1284 - val_avg_multilabel_BA_2: 0.8621\n",
      "Epoch 32/100\n",
      "3806/3806 [==============================] - 3s 856us/step - loss: 0.1243 - avg_multilabel_BA_2: 0.8621 - val_loss: 0.1274 - val_avg_multilabel_BA_2: 0.8621\n",
      "Epoch 33/100\n",
      "3806/3806 [==============================] - 3s 864us/step - loss: 0.1240 - avg_multilabel_BA_2: 0.8621 - val_loss: 0.1285 - val_avg_multilabel_BA_2: 0.8622\n",
      "Epoch 34/100\n",
      "3806/3806 [==============================] - 3s 874us/step - loss: 0.1238 - avg_multilabel_BA_2: 0.8622 - val_loss: 0.1302 - val_avg_multilabel_BA_2: 0.8622\n",
      "Epoch 35/100\n",
      "3806/3806 [==============================] - 3s 858us/step - loss: 0.1237 - avg_multilabel_BA_2: 0.8622 - val_loss: 0.1255 - val_avg_multilabel_BA_2: 0.8622\n",
      "Epoch 36/100\n",
      "3806/3806 [==============================] - 3s 861us/step - loss: 0.1233 - avg_multilabel_BA_2: 0.8623 - val_loss: 0.1272 - val_avg_multilabel_BA_2: 0.8623\n",
      "Epoch 37/100\n",
      "3806/3806 [==============================] - 3s 861us/step - loss: 0.1233 - avg_multilabel_BA_2: 0.8623 - val_loss: 0.1262 - val_avg_multilabel_BA_2: 0.8623\n",
      "Epoch 38/100\n",
      "3806/3806 [==============================] - 4s 923us/step - loss: 0.1232 - avg_multilabel_BA_2: 0.8623 - val_loss: 0.1252 - val_avg_multilabel_BA_2: 0.8624\n",
      "Epoch 39/100\n",
      "3806/3806 [==============================] - 3s 895us/step - loss: 0.1230 - avg_multilabel_BA_2: 0.8624 - val_loss: 0.1261 - val_avg_multilabel_BA_2: 0.8624\n",
      "Epoch 40/100\n",
      "3806/3806 [==============================] - 3s 898us/step - loss: 0.1228 - avg_multilabel_BA_2: 0.8624 - val_loss: 0.1259 - val_avg_multilabel_BA_2: 0.8624\n",
      "Epoch 41/100\n",
      "3806/3806 [==============================] - 3s 876us/step - loss: 0.1228 - avg_multilabel_BA_2: 0.8625 - val_loss: 0.1254 - val_avg_multilabel_BA_2: 0.8625\n",
      "Epoch 42/100\n",
      "3806/3806 [==============================] - 3s 859us/step - loss: 0.1226 - avg_multilabel_BA_2: 0.8625 - val_loss: 0.1304 - val_avg_multilabel_BA_2: 0.8625\n",
      "Epoch 43/100\n",
      "3806/3806 [==============================] - 3s 859us/step - loss: 0.1225 - avg_multilabel_BA_2: 0.8625 - val_loss: 0.1261 - val_avg_multilabel_BA_2: 0.8626\n",
      "Epoch 44/100\n",
      "3806/3806 [==============================] - 3s 845us/step - loss: 0.1223 - avg_multilabel_BA_2: 0.8626 - val_loss: 0.1251 - val_avg_multilabel_BA_2: 0.8626\n",
      "Epoch 45/100\n",
      "3806/3806 [==============================] - 3s 863us/step - loss: 0.1222 - avg_multilabel_BA_2: 0.8626 - val_loss: 0.1264 - val_avg_multilabel_BA_2: 0.8627\n",
      "Epoch 46/100\n",
      "3806/3806 [==============================] - 3s 861us/step - loss: 0.1221 - avg_multilabel_BA_2: 0.8627 - val_loss: 0.1253 - val_avg_multilabel_BA_2: 0.8627\n",
      "Epoch 47/100\n",
      "3806/3806 [==============================] - 3s 864us/step - loss: 0.1219 - avg_multilabel_BA_2: 0.8627 - val_loss: 0.1262 - val_avg_multilabel_BA_2: 0.8627\n",
      "Epoch 48/100\n",
      "3806/3806 [==============================] - 3s 874us/step - loss: 0.1218 - avg_multilabel_BA_2: 0.8628 - val_loss: 0.1260 - val_avg_multilabel_BA_2: 0.8628\n",
      "Epoch 49/100\n",
      "3806/3806 [==============================] - 3s 887us/step - loss: 0.1217 - avg_multilabel_BA_2: 0.8628 - val_loss: 0.1256 - val_avg_multilabel_BA_2: 0.8628\n",
      "Epoch 50/100\n",
      "3806/3806 [==============================] - 3s 888us/step - loss: 0.1216 - avg_multilabel_BA_2: 0.8628 - val_loss: 0.1252 - val_avg_multilabel_BA_2: 0.8628\n",
      "Epoch 51/100\n",
      "3806/3806 [==============================] - 3s 858us/step - loss: 0.1216 - avg_multilabel_BA_2: 0.8628 - val_loss: 0.1245 - val_avg_multilabel_BA_2: 0.8629\n",
      "Epoch 52/100\n",
      "3806/3806 [==============================] - 3s 845us/step - loss: 0.1214 - avg_multilabel_BA_2: 0.8630 - val_loss: 0.1241 - val_avg_multilabel_BA_2: 0.8629\n",
      "Epoch 53/100\n",
      "3806/3806 [==============================] - 3s 855us/step - loss: 0.1212 - avg_multilabel_BA_2: 0.8630 - val_loss: 0.1242 - val_avg_multilabel_BA_2: 0.8630\n",
      "Epoch 54/100\n",
      "3806/3806 [==============================] - 3s 849us/step - loss: 0.1213 - avg_multilabel_BA_2: 0.8630 - val_loss: 0.1252 - val_avg_multilabel_BA_2: 0.8630\n",
      "Epoch 55/100\n",
      "3806/3806 [==============================] - 3s 849us/step - loss: 0.1212 - avg_multilabel_BA_2: 0.8630 - val_loss: 0.1240 - val_avg_multilabel_BA_2: 0.8631\n",
      "Epoch 56/100\n",
      "3806/3806 [==============================] - 3s 847us/step - loss: 0.1211 - avg_multilabel_BA_2: 0.8631 - val_loss: 0.1248 - val_avg_multilabel_BA_2: 0.8631\n",
      "Epoch 57/100\n",
      "3806/3806 [==============================] - 3s 850us/step - loss: 0.1208 - avg_multilabel_BA_2: 0.8631 - val_loss: 0.1244 - val_avg_multilabel_BA_2: 0.8632\n",
      "Epoch 58/100\n",
      "3806/3806 [==============================] - 3s 846us/step - loss: 0.1208 - avg_multilabel_BA_2: 0.8632 - val_loss: 0.1266 - val_avg_multilabel_BA_2: 0.8632\n",
      "Epoch 59/100\n",
      "3806/3806 [==============================] - 3s 844us/step - loss: 0.1208 - avg_multilabel_BA_2: 0.8632 - val_loss: 0.1251 - val_avg_multilabel_BA_2: 0.8632\n",
      "Epoch 60/100\n",
      "3806/3806 [==============================] - 3s 842us/step - loss: 0.1207 - avg_multilabel_BA_2: 0.8633 - val_loss: 0.1260 - val_avg_multilabel_BA_2: 0.8633\n",
      "Epoch 61/100\n",
      "3806/3806 [==============================] - 3s 845us/step - loss: 0.1206 - avg_multilabel_BA_2: 0.8633 - val_loss: 0.1257 - val_avg_multilabel_BA_2: 0.8633\n",
      "Epoch 62/100\n",
      "3806/3806 [==============================] - 3s 840us/step - loss: 0.1206 - avg_multilabel_BA_2: 0.8633 - val_loss: 0.1246 - val_avg_multilabel_BA_2: 0.8634\n",
      "Epoch 63/100\n",
      "3806/3806 [==============================] - 3s 846us/step - loss: 0.1204 - avg_multilabel_BA_2: 0.8634 - val_loss: 0.1242 - val_avg_multilabel_BA_2: 0.8634\n",
      "Epoch 64/100\n",
      "3806/3806 [==============================] - 3s 841us/step - loss: 0.1203 - avg_multilabel_BA_2: 0.8635 - val_loss: 0.1240 - val_avg_multilabel_BA_2: 0.8635\n",
      "Epoch 65/100\n",
      "3806/3806 [==============================] - 3s 853us/step - loss: 0.1203 - avg_multilabel_BA_2: 0.8635 - val_loss: 0.1241 - val_avg_multilabel_BA_2: 0.8635\n",
      "Epoch 66/100\n",
      "3806/3806 [==============================] - 3s 867us/step - loss: 0.1201 - avg_multilabel_BA_2: 0.8635 - val_loss: 0.1260 - val_avg_multilabel_BA_2: 0.8636\n",
      "Epoch 67/100\n",
      "3806/3806 [==============================] - 3s 870us/step - loss: 0.1200 - avg_multilabel_BA_2: 0.8635 - val_loss: 0.1240 - val_avg_multilabel_BA_2: 0.8636\n",
      "Epoch 68/100\n",
      "3806/3806 [==============================] - 3s 871us/step - loss: 0.1200 - avg_multilabel_BA_2: 0.8636 - val_loss: 0.1257 - val_avg_multilabel_BA_2: 0.8636\n",
      "Epoch 69/100\n",
      "3806/3806 [==============================] - 3s 886us/step - loss: 0.1200 - avg_multilabel_BA_2: 0.8637 - val_loss: 0.1243 - val_avg_multilabel_BA_2: 0.8637\n",
      "Epoch 70/100\n",
      "3806/3806 [==============================] - 3s 851us/step - loss: 0.1200 - avg_multilabel_BA_2: 0.8637 - val_loss: 0.1237 - val_avg_multilabel_BA_2: 0.8637\n",
      "Epoch 71/100\n",
      "3806/3806 [==============================] - 3s 881us/step - loss: 0.1198 - avg_multilabel_BA_2: 0.8638 - val_loss: 0.1290 - val_avg_multilabel_BA_2: 0.8638\n",
      "Epoch 72/100\n",
      "3806/3806 [==============================] - 3s 900us/step - loss: 0.1197 - avg_multilabel_BA_2: 0.8638 - val_loss: 0.1242 - val_avg_multilabel_BA_2: 0.8638\n",
      "Epoch 73/100\n",
      "3806/3806 [==============================] - 3s 885us/step - loss: 0.1197 - avg_multilabel_BA_2: 0.8638 - val_loss: 0.1232 - val_avg_multilabel_BA_2: 0.8638\n",
      "Epoch 74/100\n",
      "3806/3806 [==============================] - 3s 878us/step - loss: 0.1196 - avg_multilabel_BA_2: 0.8639 - val_loss: 0.1226 - val_avg_multilabel_BA_2: 0.8639\n",
      "Epoch 75/100\n",
      "3806/3806 [==============================] - 3s 882us/step - loss: 0.1195 - avg_multilabel_BA_2: 0.8639 - val_loss: 0.1235 - val_avg_multilabel_BA_2: 0.8639\n",
      "Epoch 76/100\n",
      "3806/3806 [==============================] - 3s 878us/step - loss: 0.1194 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1256 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 77/100\n",
      "3806/3806 [==============================] - 3s 872us/step - loss: 0.1194 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1243 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 78/100\n",
      "3806/3806 [==============================] - 3s 872us/step - loss: 0.1193 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1230 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 79/100\n",
      "3806/3806 [==============================] - 3s 897us/step - loss: 0.1192 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1233 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 80/100\n",
      "3806/3806 [==============================] - 3s 873us/step - loss: 0.1192 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1240 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 81/100\n",
      "3806/3806 [==============================] - 3s 879us/step - loss: 0.1192 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1242 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 82/100\n",
      "3806/3806 [==============================] - 3s 877us/step - loss: 0.1192 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1241 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 83/100\n",
      "3806/3806 [==============================] - 3s 872us/step - loss: 0.1191 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1244 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 84/100\n",
      "3806/3806 [==============================] - 3s 871us/step - loss: 0.1189 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1250 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 85/100\n",
      "3806/3806 [==============================] - 3s 873us/step - loss: 0.1189 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1238 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 86/100\n",
      "3806/3806 [==============================] - 3s 874us/step - loss: 0.1189 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1222 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 87/100\n",
      "3806/3806 [==============================] - 3s 884us/step - loss: 0.1187 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1251 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 88/100\n",
      "3806/3806 [==============================] - 3s 867us/step - loss: 0.1186 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1232 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 89/100\n",
      "3806/3806 [==============================] - 3s 884us/step - loss: 0.1187 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1224 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 90/100\n",
      "3806/3806 [==============================] - 3s 884us/step - loss: 0.1187 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1257 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 91/100\n",
      "3806/3806 [==============================] - 3s 873us/step - loss: 0.1187 - avg_multilabel_BA_2: 0.8646 - val_loss: 0.1229 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 92/100\n",
      "3806/3806 [==============================] - 3s 869us/step - loss: 0.1186 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1230 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 93/100\n",
      "3806/3806 [==============================] - 3s 874us/step - loss: 0.1184 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1233 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 94/100\n",
      "3806/3806 [==============================] - 3s 870us/step - loss: 0.1184 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1225 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 95/100\n",
      "3806/3806 [==============================] - 3s 869us/step - loss: 0.1182 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1251 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 96/100\n",
      "3806/3806 [==============================] - 3s 877us/step - loss: 0.1182 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1225 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 97/100\n",
      "3806/3806 [==============================] - 3s 872us/step - loss: 0.1181 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1223 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 98/100\n",
      "3806/3806 [==============================] - 3s 873us/step - loss: 0.1182 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1239 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 99/100\n",
      "3806/3806 [==============================] - 3s 868us/step - loss: 0.1180 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1229 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 100/100\n",
      "3806/3806 [==============================] - 3s 852us/step - loss: 0.1181 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1221 - val_avg_multilabel_BA_2: 0.8650\n",
      "Best epoch: 100\n",
      "Epoch 1/100\n",
      "3806/3806 [==============================] - 4s 967us/step - loss: 0.2000 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1713 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 2/100\n",
      "3806/3806 [==============================] - 3s 874us/step - loss: 0.1624 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1628 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 3/100\n",
      "3806/3806 [==============================] - 3s 869us/step - loss: 0.1543 - avg_multilabel_BA_2: 0.8646 - val_loss: 0.1538 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 4/100\n",
      "3806/3806 [==============================] - 3s 872us/step - loss: 0.1496 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1481 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 5/100\n",
      "3806/3806 [==============================] - 3s 871us/step - loss: 0.1463 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1472 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 6/100\n",
      "3806/3806 [==============================] - 3s 870us/step - loss: 0.1437 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1454 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 7/100\n",
      "3806/3806 [==============================] - 3s 869us/step - loss: 0.1419 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1420 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 8/100\n",
      "3806/3806 [==============================] - 3s 868us/step - loss: 0.1403 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1489 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 9/100\n",
      "3806/3806 [==============================] - 3s 869us/step - loss: 0.1390 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1402 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 10/100\n",
      "3806/3806 [==============================] - 3s 873us/step - loss: 0.1379 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1414 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 11/100\n",
      "3806/3806 [==============================] - 3s 877us/step - loss: 0.1370 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1401 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 12/100\n",
      "3806/3806 [==============================] - 3s 838us/step - loss: 0.1360 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1368 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 13/100\n",
      "3806/3806 [==============================] - 3s 843us/step - loss: 0.1352 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1367 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 14/100\n",
      "3806/3806 [==============================] - 3s 837us/step - loss: 0.1346 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1365 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 15/100\n",
      "3806/3806 [==============================] - 3s 858us/step - loss: 0.1339 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1411 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 16/100\n",
      "3806/3806 [==============================] - 3s 840us/step - loss: 0.1333 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1354 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 17/100\n",
      "3806/3806 [==============================] - 3s 839us/step - loss: 0.1328 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1349 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 18/100\n",
      "3806/3806 [==============================] - 3s 839us/step - loss: 0.1323 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1341 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 19/100\n",
      "3806/3806 [==============================] - 3s 837us/step - loss: 0.1318 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1353 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 20/100\n",
      "3806/3806 [==============================] - 3s 841us/step - loss: 0.1315 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1410 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 21/100\n",
      "3806/3806 [==============================] - 3s 838us/step - loss: 0.1313 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1336 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 22/100\n",
      "3806/3806 [==============================] - 3s 842us/step - loss: 0.1309 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1379 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 23/100\n",
      "3806/3806 [==============================] - 3s 834us/step - loss: 0.1303 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1319 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 24/100\n",
      "3806/3806 [==============================] - 3s 843us/step - loss: 0.1300 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1326 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 25/100\n",
      "3806/3806 [==============================] - 3s 841us/step - loss: 0.1297 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1318 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 26/100\n",
      "3806/3806 [==============================] - 3s 837us/step - loss: 0.1295 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1319 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 27/100\n",
      "3806/3806 [==============================] - 3s 875us/step - loss: 0.1291 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1304 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 28/100\n",
      "3806/3806 [==============================] - 3s 869us/step - loss: 0.1289 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1318 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 29/100\n",
      "3806/3806 [==============================] - 3s 878us/step - loss: 0.1286 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1319 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 30/100\n",
      "3806/3806 [==============================] - 3s 872us/step - loss: 0.1284 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1304 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 31/100\n",
      "3806/3806 [==============================] - 3s 869us/step - loss: 0.1281 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1332 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 32/100\n",
      "3806/3806 [==============================] - 3s 865us/step - loss: 0.1279 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1295 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 33/100\n",
      "3806/3806 [==============================] - 3s 871us/step - loss: 0.1276 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1304 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 34/100\n",
      "3806/3806 [==============================] - 3s 864us/step - loss: 0.1273 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1291 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 35/100\n",
      "3806/3806 [==============================] - 3s 861us/step - loss: 0.1272 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1305 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 36/100\n",
      "3806/3806 [==============================] - 3s 842us/step - loss: 0.1270 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1297 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 37/100\n",
      "3806/3806 [==============================] - 3s 835us/step - loss: 0.1267 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1294 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 38/100\n",
      "3806/3806 [==============================] - 3s 840us/step - loss: 0.1266 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1300 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 39/100\n",
      "3806/3806 [==============================] - 3s 868us/step - loss: 0.1264 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1289 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 40/100\n",
      "3806/3806 [==============================] - 3s 865us/step - loss: 0.1262 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1295 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 41/100\n",
      "3806/3806 [==============================] - 3s 865us/step - loss: 0.1260 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1312 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 42/100\n",
      "3806/3806 [==============================] - 3s 869us/step - loss: 0.1257 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1283 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 43/100\n",
      "3806/3806 [==============================] - 3s 869us/step - loss: 0.1258 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1280 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 44/100\n",
      "3806/3806 [==============================] - 3s 873us/step - loss: 0.1256 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1276 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 45/100\n",
      "3806/3806 [==============================] - 3s 873us/step - loss: 0.1255 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1283 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 46/100\n",
      "3806/3806 [==============================] - 3s 867us/step - loss: 0.1252 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1272 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 47/100\n",
      "3806/3806 [==============================] - 3s 871us/step - loss: 0.1250 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1301 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 48/100\n",
      "3806/3806 [==============================] - 3s 860us/step - loss: 0.1249 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1285 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 49/100\n",
      "3806/3806 [==============================] - 3s 837us/step - loss: 0.1248 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1273 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 50/100\n",
      "3806/3806 [==============================] - 3s 839us/step - loss: 0.1246 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1302 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 51/100\n",
      "3806/3806 [==============================] - 3s 838us/step - loss: 0.1246 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1277 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 52/100\n",
      "3806/3806 [==============================] - 3s 836us/step - loss: 0.1243 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1290 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 53/100\n",
      "3806/3806 [==============================] - 3s 853us/step - loss: 0.1242 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1282 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 54/100\n",
      "3806/3806 [==============================] - 3s 872us/step - loss: 0.1242 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1266 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 55/100\n",
      "3806/3806 [==============================] - 3s 871us/step - loss: 0.1239 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1286 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 56/100\n",
      "3806/3806 [==============================] - 3s 866us/step - loss: 0.1239 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1262 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 57/100\n",
      "3806/3806 [==============================] - 3s 869us/step - loss: 0.1237 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1268 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 58/100\n",
      "3806/3806 [==============================] - 3s 866us/step - loss: 0.1237 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1261 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 59/100\n",
      "3806/3806 [==============================] - 3s 870us/step - loss: 0.1235 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1268 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 60/100\n",
      "3806/3806 [==============================] - 3s 865us/step - loss: 0.1234 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1269 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 61/100\n",
      "3806/3806 [==============================] - 3s 867us/step - loss: 0.1232 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1262 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 62/100\n",
      "3806/3806 [==============================] - 3s 873us/step - loss: 0.1233 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1270 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 63/100\n",
      "3806/3806 [==============================] - 3s 871us/step - loss: 0.1231 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1277 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 64/100\n",
      "3806/3806 [==============================] - 3s 877us/step - loss: 0.1229 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1262 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 65/100\n",
      "3806/3806 [==============================] - 3s 854us/step - loss: 0.1228 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1258 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 66/100\n",
      "3806/3806 [==============================] - 3s 843us/step - loss: 0.1228 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1250 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 67/100\n",
      "3806/3806 [==============================] - 3s 834us/step - loss: 0.1228 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1265 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 68/100\n",
      "3806/3806 [==============================] - 3s 820us/step - loss: 0.1226 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1274 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 69/100\n",
      "3806/3806 [==============================] - 3s 818us/step - loss: 0.1225 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1255 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 70/100\n",
      "3806/3806 [==============================] - 3s 823us/step - loss: 0.1223 - avg_multilabel_BA_2: 0.8646 - val_loss: 0.1257 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 71/100\n",
      "3806/3806 [==============================] - 3s 819us/step - loss: 0.1224 - avg_multilabel_BA_2: 0.8646 - val_loss: 0.1265 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 72/100\n",
      "3806/3806 [==============================] - 3s 819us/step - loss: 0.1223 - avg_multilabel_BA_2: 0.8646 - val_loss: 0.1259 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 73/100\n",
      "3806/3806 [==============================] - 3s 820us/step - loss: 0.1222 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1270 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 74/100\n",
      "3806/3806 [==============================] - 3s 819us/step - loss: 0.1222 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1259 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 75/100\n",
      "3806/3806 [==============================] - 3s 820us/step - loss: 0.1219 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1276 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 76/100\n",
      "3806/3806 [==============================] - 3s 817us/step - loss: 0.1219 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1296 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 77/100\n",
      "3806/3806 [==============================] - 3s 820us/step - loss: 0.1220 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1290 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 78/100\n",
      "3806/3806 [==============================] - 3s 843us/step - loss: 0.1217 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1235 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 79/100\n",
      "3806/3806 [==============================] - 3s 856us/step - loss: 0.1217 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1247 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 80/100\n",
      "3806/3806 [==============================] - 3s 855us/step - loss: 0.1218 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1284 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 81/100\n",
      "3806/3806 [==============================] - 3s 850us/step - loss: 0.1215 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1261 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 82/100\n",
      "3806/3806 [==============================] - 3s 846us/step - loss: 0.1214 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1244 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 83/100\n",
      "3806/3806 [==============================] - 3s 851us/step - loss: 0.1215 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1294 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 84/100\n",
      "3806/3806 [==============================] - 3s 853us/step - loss: 0.1214 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1242 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 85/100\n",
      "3806/3806 [==============================] - 3s 854us/step - loss: 0.1215 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1241 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 86/100\n",
      "3806/3806 [==============================] - 3s 854us/step - loss: 0.1212 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1248 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 87/100\n",
      "3806/3806 [==============================] - 3s 854us/step - loss: 0.1213 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1245 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 88/100\n",
      "3806/3806 [==============================] - 3s 855us/step - loss: 0.1212 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1244 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 89/100\n",
      "3806/3806 [==============================] - 3s 857us/step - loss: 0.1211 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1258 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 90/100\n",
      "3806/3806 [==============================] - 3s 855us/step - loss: 0.1211 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1241 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 91/100\n",
      "3806/3806 [==============================] - 3s 855us/step - loss: 0.1210 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1257 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 92/100\n",
      "3806/3806 [==============================] - 3s 855us/step - loss: 0.1209 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1233 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 93/100\n",
      "3806/3806 [==============================] - 3s 851us/step - loss: 0.1209 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1239 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 94/100\n",
      "3806/3806 [==============================] - 3s 844us/step - loss: 0.1207 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1246 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 95/100\n",
      "3806/3806 [==============================] - 3s 845us/step - loss: 0.1207 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1241 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 96/100\n",
      "3806/3806 [==============================] - 3s 854us/step - loss: 0.1207 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1233 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 97/100\n",
      "3806/3806 [==============================] - 3s 850us/step - loss: 0.1205 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1275 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 98/100\n",
      "3806/3806 [==============================] - 3s 843us/step - loss: 0.1207 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1239 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 99/100\n",
      "3806/3806 [==============================] - 3s 869us/step - loss: 0.1205 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1274 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 100/100\n",
      "3806/3806 [==============================] - 3s 852us/step - loss: 0.1206 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1251 - val_avg_multilabel_BA_2: 0.8652\n",
      "1859/1859 [==============================] - 1s 543us/step - loss: 0.1253 - avg_multilabel_BA_2: 0.8652\n",
      "Test results - Loss: 0.1252952367067337 - Averaged Balanced Accuracy: 0.8652029633522034%\n",
      "Averaged Balanced Accuracy: 0.865198\n",
      "INFO:tensorflow:Assets written to: model/saved_model_fold_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/saved_model_fold_1/assets\n",
      "2022-04-11 16:35:07.683360: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2022-04-11 16:35:07.683380: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "2022-04-11 16:35:07.683384: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:372] Ignored change_concat_input_ranges.\n",
      "2022-04-11 16:35:07.683557: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: model/saved_model_fold_1\n",
      "2022-04-11 16:35:07.684302: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-04-11 16:35:07.684314: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: model/saved_model_fold_1\n",
      "2022-04-11 16:35:07.686424: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-04-11 16:35:07.713407: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: model/saved_model_fold_1\n",
      "2022-04-11 16:35:07.721978: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 38422 microseconds.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project hypertunning/experimento_1_RandSearchCV/oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project hypertunning/experimento_1_RandSearchCV/oracle.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from hypertunning/experimento_1_RandSearchCV/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from hypertunning/experimento_1_RandSearchCV/tuner0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3876/3876 [==============================] - 4s 947us/step - loss: 0.2001 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1728 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 2/100\n",
      "3876/3876 [==============================] - 3s 858us/step - loss: 0.1641 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1584 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 3/100\n",
      "3876/3876 [==============================] - 3s 871us/step - loss: 0.1557 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1558 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 4/100\n",
      "3876/3876 [==============================] - 3s 870us/step - loss: 0.1514 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1497 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 5/100\n",
      "3876/3876 [==============================] - 3s 859us/step - loss: 0.1481 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1507 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 6/100\n",
      "3876/3876 [==============================] - 3s 859us/step - loss: 0.1460 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1466 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 7/100\n",
      "3876/3876 [==============================] - 3s 861us/step - loss: 0.1440 - avg_multilabel_BA_2: 0.8646 - val_loss: 0.1431 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 8/100\n",
      "3876/3876 [==============================] - 3s 859us/step - loss: 0.1422 - avg_multilabel_BA_2: 0.8646 - val_loss: 0.1426 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 9/100\n",
      "3876/3876 [==============================] - 3s 858us/step - loss: 0.1409 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1416 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 10/100\n",
      "3876/3876 [==============================] - 3s 862us/step - loss: 0.1397 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1402 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 11/100\n",
      "3876/3876 [==============================] - 3s 863us/step - loss: 0.1386 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1390 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 12/100\n",
      "3876/3876 [==============================] - 3s 885us/step - loss: 0.1376 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1388 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 13/100\n",
      "3876/3876 [==============================] - 3s 862us/step - loss: 0.1366 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1396 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 14/100\n",
      "3876/3876 [==============================] - 3s 858us/step - loss: 0.1360 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1372 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 15/100\n",
      "3876/3876 [==============================] - 3s 857us/step - loss: 0.1352 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1367 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 16/100\n",
      "3876/3876 [==============================] - 3s 856us/step - loss: 0.1345 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1358 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 17/100\n",
      "3876/3876 [==============================] - 3s 859us/step - loss: 0.1340 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1372 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 18/100\n",
      "3876/3876 [==============================] - 3s 859us/step - loss: 0.1335 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1374 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 19/100\n",
      "3876/3876 [==============================] - 3s 856us/step - loss: 0.1331 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1352 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 20/100\n",
      "3876/3876 [==============================] - 3s 858us/step - loss: 0.1328 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1336 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 21/100\n",
      "3876/3876 [==============================] - 3s 855us/step - loss: 0.1322 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1346 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 22/100\n",
      "3876/3876 [==============================] - 3s 858us/step - loss: 0.1318 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1334 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 23/100\n",
      "3876/3876 [==============================] - 3s 854us/step - loss: 0.1316 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1330 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 24/100\n",
      "3876/3876 [==============================] - 3s 853us/step - loss: 0.1312 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1333 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 25/100\n",
      "3876/3876 [==============================] - 3s 857us/step - loss: 0.1308 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1378 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 26/100\n",
      "3876/3876 [==============================] - 3s 858us/step - loss: 0.1305 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1332 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 27/100\n",
      "3876/3876 [==============================] - 3s 859us/step - loss: 0.1303 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1340 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 28/100\n",
      "3876/3876 [==============================] - 3s 859us/step - loss: 0.1302 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1327 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 29/100\n",
      "3876/3876 [==============================] - 3s 856us/step - loss: 0.1299 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1367 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 30/100\n",
      "3876/3876 [==============================] - 3s 860us/step - loss: 0.1296 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1318 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 31/100\n",
      "3876/3876 [==============================] - 3s 859us/step - loss: 0.1293 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1320 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 32/100\n",
      "3876/3876 [==============================] - 3s 857us/step - loss: 0.1292 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1319 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 33/100\n",
      "3876/3876 [==============================] - 3s 861us/step - loss: 0.1289 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1315 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 34/100\n",
      "3876/3876 [==============================] - 3s 860us/step - loss: 0.1287 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1320 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 35/100\n",
      "3876/3876 [==============================] - 3s 858us/step - loss: 0.1288 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1313 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 36/100\n",
      "3876/3876 [==============================] - 3s 856us/step - loss: 0.1284 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1301 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 37/100\n",
      "3876/3876 [==============================] - 3s 857us/step - loss: 0.1282 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1336 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 38/100\n",
      "3876/3876 [==============================] - 3s 858us/step - loss: 0.1281 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1311 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 39/100\n",
      "3876/3876 [==============================] - 3s 858us/step - loss: 0.1278 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1302 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 40/100\n",
      "3876/3876 [==============================] - 3s 859us/step - loss: 0.1277 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1306 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 41/100\n",
      "3876/3876 [==============================] - 3s 851us/step - loss: 0.1275 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1347 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 42/100\n",
      "3876/3876 [==============================] - 3s 860us/step - loss: 0.1275 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1298 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 43/100\n",
      "3876/3876 [==============================] - 3s 855us/step - loss: 0.1273 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1294 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 44/100\n",
      "3876/3876 [==============================] - 3s 867us/step - loss: 0.1272 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1306 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 45/100\n",
      "3876/3876 [==============================] - 3s 857us/step - loss: 0.1271 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1302 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 46/100\n",
      "3876/3876 [==============================] - 3s 862us/step - loss: 0.1270 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1304 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 47/100\n",
      "3876/3876 [==============================] - 3s 866us/step - loss: 0.1268 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1310 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 48/100\n",
      "3876/3876 [==============================] - 3s 855us/step - loss: 0.1270 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1290 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 49/100\n",
      "3876/3876 [==============================] - 3s 858us/step - loss: 0.1266 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1306 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 50/100\n",
      "3876/3876 [==============================] - 3s 860us/step - loss: 0.1266 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1294 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 51/100\n",
      "3876/3876 [==============================] - 3s 856us/step - loss: 0.1264 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1283 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 52/100\n",
      "3876/3876 [==============================] - 3s 858us/step - loss: 0.1264 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1283 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 53/100\n",
      "3876/3876 [==============================] - 3s 854us/step - loss: 0.1262 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1290 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 54/100\n",
      "3876/3876 [==============================] - 3s 866us/step - loss: 0.1262 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1286 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 55/100\n",
      "3876/3876 [==============================] - 3s 859us/step - loss: 0.1260 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1283 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 56/100\n",
      "3876/3876 [==============================] - 3s 857us/step - loss: 0.1260 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1281 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 57/100\n",
      "3876/3876 [==============================] - 3s 884us/step - loss: 0.1258 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1296 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 58/100\n",
      "3876/3876 [==============================] - 3s 877us/step - loss: 0.1258 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1338 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 59/100\n",
      "3876/3876 [==============================] - 3s 875us/step - loss: 0.1258 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1324 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 60/100\n",
      "3876/3876 [==============================] - 3s 871us/step - loss: 0.1256 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1303 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 61/100\n",
      "3876/3876 [==============================] - 3s 867us/step - loss: 0.1255 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1305 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 62/100\n",
      "3876/3876 [==============================] - 3s 871us/step - loss: 0.1254 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1301 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 63/100\n",
      "3876/3876 [==============================] - 3s 872us/step - loss: 0.1254 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1275 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 64/100\n",
      "3876/3876 [==============================] - 3s 869us/step - loss: 0.1254 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1285 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 65/100\n",
      "3876/3876 [==============================] - 3s 865us/step - loss: 0.1253 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1285 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 66/100\n",
      "3876/3876 [==============================] - 3s 863us/step - loss: 0.1250 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1303 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 67/100\n",
      "3876/3876 [==============================] - 3s 868us/step - loss: 0.1250 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1283 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 68/100\n",
      "3876/3876 [==============================] - 3s 868us/step - loss: 0.1251 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1286 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 69/100\n",
      "3876/3876 [==============================] - 3s 871us/step - loss: 0.1249 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1283 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 70/100\n",
      "3876/3876 [==============================] - 3s 855us/step - loss: 0.1249 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1285 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 71/100\n",
      "3876/3876 [==============================] - 3s 854us/step - loss: 0.1247 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1290 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 72/100\n",
      "3876/3876 [==============================] - 3s 860us/step - loss: 0.1247 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1299 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 73/100\n",
      "3876/3876 [==============================] - 3s 859us/step - loss: 0.1246 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1317 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 74/100\n",
      "3876/3876 [==============================] - 3s 853us/step - loss: 0.1247 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1302 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 75/100\n",
      "3876/3876 [==============================] - 3s 855us/step - loss: 0.1245 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1285 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 76/100\n",
      "3876/3876 [==============================] - 3s 856us/step - loss: 0.1246 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1271 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 77/100\n",
      "3876/3876 [==============================] - 3s 851us/step - loss: 0.1246 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1311 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 78/100\n",
      "3876/3876 [==============================] - 3s 851us/step - loss: 0.1245 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1282 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 79/100\n",
      "3876/3876 [==============================] - 3s 854us/step - loss: 0.1244 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1271 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 80/100\n",
      "3876/3876 [==============================] - 3s 854us/step - loss: 0.1242 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1299 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 81/100\n",
      "3876/3876 [==============================] - 3s 857us/step - loss: 0.1242 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1279 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 82/100\n",
      "3876/3876 [==============================] - 3s 855us/step - loss: 0.1242 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1288 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 83/100\n",
      "3876/3876 [==============================] - 3s 859us/step - loss: 0.1240 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1295 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 84/100\n",
      "3876/3876 [==============================] - 3s 860us/step - loss: 0.1241 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1275 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 85/100\n",
      "3876/3876 [==============================] - 3s 857us/step - loss: 0.1239 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1293 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 86/100\n",
      "3876/3876 [==============================] - 3s 854us/step - loss: 0.1239 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1283 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 87/100\n",
      "3876/3876 [==============================] - 3s 856us/step - loss: 0.1239 - avg_multilabel_BA_2: 0.8646 - val_loss: 0.1283 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 88/100\n",
      "3876/3876 [==============================] - 3s 860us/step - loss: 0.1238 - avg_multilabel_BA_2: 0.8646 - val_loss: 0.1282 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 89/100\n",
      "3876/3876 [==============================] - 3s 889us/step - loss: 0.1238 - avg_multilabel_BA_2: 0.8646 - val_loss: 0.1288 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 90/100\n",
      "3876/3876 [==============================] - 3s 874us/step - loss: 0.1238 - avg_multilabel_BA_2: 0.8646 - val_loss: 0.1286 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 91/100\n",
      "3876/3876 [==============================] - 3s 864us/step - loss: 0.1236 - avg_multilabel_BA_2: 0.8646 - val_loss: 0.1278 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 92/100\n",
      "3876/3876 [==============================] - 3s 844us/step - loss: 0.1236 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1263 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 93/100\n",
      "3876/3876 [==============================] - 3s 838us/step - loss: 0.1235 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1267 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 94/100\n",
      "3876/3876 [==============================] - 3s 861us/step - loss: 0.1234 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1275 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 95/100\n",
      "3876/3876 [==============================] - 3s 868us/step - loss: 0.1235 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1279 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 96/100\n",
      "3876/3876 [==============================] - 3s 866us/step - loss: 0.1234 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1262 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 97/100\n",
      "3876/3876 [==============================] - 3s 867us/step - loss: 0.1233 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1269 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 98/100\n",
      "3876/3876 [==============================] - 3s 863us/step - loss: 0.1232 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1277 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 99/100\n",
      "3876/3876 [==============================] - 3s 867us/step - loss: 0.1231 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1304 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 100/100\n",
      "3876/3876 [==============================] - 3s 855us/step - loss: 0.1231 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1304 - val_avg_multilabel_BA_2: 0.8647\n",
      "Best epoch: 1\n",
      "3876/3876 [==============================] - 4s 946us/step - loss: 0.2051 - avg_multilabel_BA_2: 0.8646 - val_loss: 0.1802 - val_avg_multilabel_BA_2: 0.8646\n",
      "1893/1893 [==============================] - 1s 552us/step - loss: 0.1793 - avg_multilabel_BA_2: 0.8645\n",
      "Test results - Loss: 0.17925933003425598 - Averaged Balanced Accuracy: 0.8645367622375488%\n",
      "Averaged Balanced Accuracy: 0.864508\n",
      "INFO:tensorflow:Assets written to: model/saved_model_fold_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/saved_model_fold_2/assets\n",
      "2022-04-11 16:40:58.942891: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2022-04-11 16:40:58.942912: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "2022-04-11 16:40:58.942916: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:372] Ignored change_concat_input_ranges.\n",
      "2022-04-11 16:40:58.943096: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: model/saved_model_fold_2\n",
      "2022-04-11 16:40:58.943833: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-04-11 16:40:58.943845: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: model/saved_model_fold_2\n",
      "2022-04-11 16:40:58.945933: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-04-11 16:40:58.972424: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: model/saved_model_fold_2\n",
      "2022-04-11 16:40:58.981465: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 38370 microseconds.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project hypertunning/experimento_1_RandSearchCV/oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project hypertunning/experimento_1_RandSearchCV/oracle.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from hypertunning/experimento_1_RandSearchCV/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from hypertunning/experimento_1_RandSearchCV/tuner0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3774/3774 [==============================] - 4s 953us/step - loss: 0.1993 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1748 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 2/100\n",
      "3774/3774 [==============================] - 3s 860us/step - loss: 0.1684 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1605 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 3/100\n",
      "3774/3774 [==============================] - 3s 881us/step - loss: 0.1589 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1539 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 4/100\n",
      "3774/3774 [==============================] - 3s 862us/step - loss: 0.1537 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1504 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 5/100\n",
      "3774/3774 [==============================] - 3s 862us/step - loss: 0.1501 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1472 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 6/100\n",
      "3774/3774 [==============================] - 3s 863us/step - loss: 0.1474 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1462 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 7/100\n",
      "3774/3774 [==============================] - 3s 861us/step - loss: 0.1452 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1442 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 8/100\n",
      "3774/3774 [==============================] - 3s 865us/step - loss: 0.1435 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1428 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 9/100\n",
      "3774/3774 [==============================] - 3s 863us/step - loss: 0.1420 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1426 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 10/100\n",
      "3774/3774 [==============================] - 3s 861us/step - loss: 0.1409 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1396 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 11/100\n",
      "3774/3774 [==============================] - 3s 862us/step - loss: 0.1398 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1393 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 12/100\n",
      "3774/3774 [==============================] - 3s 864us/step - loss: 0.1387 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1404 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 13/100\n",
      "3774/3774 [==============================] - 3s 865us/step - loss: 0.1378 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1381 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 14/100\n",
      "3774/3774 [==============================] - 3s 861us/step - loss: 0.1369 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1418 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 15/100\n",
      "3774/3774 [==============================] - 3s 863us/step - loss: 0.1361 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1377 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 16/100\n",
      "3774/3774 [==============================] - 3s 862us/step - loss: 0.1354 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1369 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 17/100\n",
      "3774/3774 [==============================] - 3s 863us/step - loss: 0.1346 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1358 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 18/100\n",
      "3774/3774 [==============================] - 3s 868us/step - loss: 0.1339 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1370 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 19/100\n",
      "3774/3774 [==============================] - 3s 861us/step - loss: 0.1335 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1347 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 20/100\n",
      "3774/3774 [==============================] - 3s 864us/step - loss: 0.1330 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1348 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 21/100\n",
      "3774/3774 [==============================] - 3s 866us/step - loss: 0.1325 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1366 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 22/100\n",
      "3774/3774 [==============================] - 3s 864us/step - loss: 0.1319 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1350 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 23/100\n",
      "3774/3774 [==============================] - 3s 864us/step - loss: 0.1315 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1328 - val_avg_multilabel_BA_2: 0.8640\n",
      "Epoch 24/100\n",
      "3774/3774 [==============================] - 3s 860us/step - loss: 0.1311 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1322 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 25/100\n",
      "3774/3774 [==============================] - 3s 862us/step - loss: 0.1308 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1332 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 26/100\n",
      "3774/3774 [==============================] - 3s 863us/step - loss: 0.1305 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1324 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 27/100\n",
      "3774/3774 [==============================] - 3s 862us/step - loss: 0.1301 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1320 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 28/100\n",
      "3774/3774 [==============================] - 3s 863us/step - loss: 0.1298 - avg_multilabel_BA_2: 0.8640 - val_loss: 0.1314 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 29/100\n",
      "3774/3774 [==============================] - 3s 879us/step - loss: 0.1296 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1316 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 30/100\n",
      "3774/3774 [==============================] - 3s 900us/step - loss: 0.1292 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1325 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 31/100\n",
      "3774/3774 [==============================] - 3s 888us/step - loss: 0.1290 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1309 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 32/100\n",
      "3774/3774 [==============================] - 3s 915us/step - loss: 0.1287 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1367 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 33/100\n",
      "3774/3774 [==============================] - 4s 948us/step - loss: 0.1286 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1316 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 34/100\n",
      "3774/3774 [==============================] - 3s 923us/step - loss: 0.1283 - avg_multilabel_BA_2: 0.8641 - val_loss: 0.1308 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 35/100\n",
      "3774/3774 [==============================] - 3s 925us/step - loss: 0.1280 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1311 - val_avg_multilabel_BA_2: 0.8641\n",
      "Epoch 36/100\n",
      "3774/3774 [==============================] - 3s 903us/step - loss: 0.1279 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1315 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 37/100\n",
      "3774/3774 [==============================] - 3s 902us/step - loss: 0.1276 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1305 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 38/100\n",
      "3774/3774 [==============================] - 3s 910us/step - loss: 0.1274 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1294 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 39/100\n",
      "3774/3774 [==============================] - 3s 899us/step - loss: 0.1273 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1293 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 40/100\n",
      "3774/3774 [==============================] - 3s 885us/step - loss: 0.1270 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1320 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 41/100\n",
      "3774/3774 [==============================] - 3s 900us/step - loss: 0.1269 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1323 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 42/100\n",
      "3774/3774 [==============================] - 3s 889us/step - loss: 0.1268 - avg_multilabel_BA_2: 0.8642 - val_loss: 0.1313 - val_avg_multilabel_BA_2: 0.8642\n",
      "Epoch 43/100\n",
      "3774/3774 [==============================] - 3s 895us/step - loss: 0.1266 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1296 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 44/100\n",
      "3774/3774 [==============================] - 3s 887us/step - loss: 0.1263 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1307 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 45/100\n",
      "3774/3774 [==============================] - 3s 895us/step - loss: 0.1262 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1280 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 46/100\n",
      "3774/3774 [==============================] - 3s 897us/step - loss: 0.1261 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1312 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 47/100\n",
      "3774/3774 [==============================] - 3s 898us/step - loss: 0.1259 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1293 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 48/100\n",
      "3774/3774 [==============================] - 3s 894us/step - loss: 0.1259 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1287 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 49/100\n",
      "3774/3774 [==============================] - 4s 1ms/step - loss: 0.1255 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1324 - val_avg_multilabel_BA_2: 0.8643\n",
      "Epoch 50/100\n",
      "3774/3774 [==============================] - 3s 905us/step - loss: 0.1254 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1276 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 51/100\n",
      "3774/3774 [==============================] - 3s 916us/step - loss: 0.1255 - avg_multilabel_BA_2: 0.8643 - val_loss: 0.1302 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 52/100\n",
      "3774/3774 [==============================] - 3s 911us/step - loss: 0.1252 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1318 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 53/100\n",
      "3774/3774 [==============================] - 3s 852us/step - loss: 0.1253 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1308 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 54/100\n",
      "3774/3774 [==============================] - 3s 896us/step - loss: 0.1250 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1369 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 55/100\n",
      "3774/3774 [==============================] - 3s 888us/step - loss: 0.1249 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1280 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 56/100\n",
      "3774/3774 [==============================] - 3s 887us/step - loss: 0.1248 - avg_multilabel_BA_2: 0.8644 - val_loss: 0.1289 - val_avg_multilabel_BA_2: 0.8644\n",
      "Epoch 57/100\n",
      "3774/3774 [==============================] - 3s 911us/step - loss: 0.1247 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1296 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 58/100\n",
      "3774/3774 [==============================] - 3s 912us/step - loss: 0.1245 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1294 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 59/100\n",
      "3774/3774 [==============================] - 4s 943us/step - loss: 0.1245 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1278 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 60/100\n",
      "3774/3774 [==============================] - 4s 928us/step - loss: 0.1245 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1282 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 61/100\n",
      "3774/3774 [==============================] - 4s 968us/step - loss: 0.1244 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1277 - val_avg_multilabel_BA_2: 0.8645\n",
      "Epoch 62/100\n",
      "3774/3774 [==============================] - 3s 901us/step - loss: 0.1241 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1270 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 63/100\n",
      "3774/3774 [==============================] - 3s 892us/step - loss: 0.1240 - avg_multilabel_BA_2: 0.8645 - val_loss: 0.1289 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 64/100\n",
      "3774/3774 [==============================] - 3s 914us/step - loss: 0.1238 - avg_multilabel_BA_2: 0.8646 - val_loss: 0.1295 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 65/100\n",
      "3774/3774 [==============================] - 3s 916us/step - loss: 0.1240 - avg_multilabel_BA_2: 0.8646 - val_loss: 0.1280 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 66/100\n",
      "3774/3774 [==============================] - 3s 893us/step - loss: 0.1237 - avg_multilabel_BA_2: 0.8646 - val_loss: 0.1272 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 67/100\n",
      "3774/3774 [==============================] - 3s 905us/step - loss: 0.1238 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1298 - val_avg_multilabel_BA_2: 0.8646\n",
      "Epoch 68/100\n",
      "3774/3774 [==============================] - 3s 888us/step - loss: 0.1235 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1267 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 69/100\n",
      "3774/3774 [==============================] - 3s 900us/step - loss: 0.1234 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1266 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 70/100\n",
      "3774/3774 [==============================] - 3s 924us/step - loss: 0.1234 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1291 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 71/100\n",
      "3774/3774 [==============================] - 3s 907us/step - loss: 0.1234 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1319 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 72/100\n",
      "3774/3774 [==============================] - 3s 915us/step - loss: 0.1231 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1273 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 73/100\n",
      "3774/3774 [==============================] - 3s 906us/step - loss: 0.1232 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1265 - val_avg_multilabel_BA_2: 0.8647\n",
      "Epoch 74/100\n",
      "3774/3774 [==============================] - 4s 929us/step - loss: 0.1231 - avg_multilabel_BA_2: 0.8647 - val_loss: 0.1271 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 75/100\n",
      "3774/3774 [==============================] - 3s 893us/step - loss: 0.1230 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1268 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 76/100\n",
      "3774/3774 [==============================] - 3s 886us/step - loss: 0.1231 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1275 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 77/100\n",
      "3774/3774 [==============================] - 3s 894us/step - loss: 0.1228 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1266 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 78/100\n",
      "3774/3774 [==============================] - 3s 893us/step - loss: 0.1227 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1274 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 79/100\n",
      "3774/3774 [==============================] - 3s 918us/step - loss: 0.1227 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1283 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 80/100\n",
      "3774/3774 [==============================] - 3s 892us/step - loss: 0.1225 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1278 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 81/100\n",
      "3774/3774 [==============================] - 3s 889us/step - loss: 0.1226 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1275 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 82/100\n",
      "3774/3774 [==============================] - 3s 877us/step - loss: 0.1225 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1298 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 83/100\n",
      "3774/3774 [==============================] - 3s 881us/step - loss: 0.1223 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1264 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 84/100\n",
      "3774/3774 [==============================] - 3s 889us/step - loss: 0.1223 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1271 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 85/100\n",
      "3774/3774 [==============================] - 3s 879us/step - loss: 0.1221 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1264 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 86/100\n",
      "3774/3774 [==============================] - 3s 876us/step - loss: 0.1222 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1271 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 87/100\n",
      "3774/3774 [==============================] - 3s 885us/step - loss: 0.1222 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1277 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 88/100\n",
      "3774/3774 [==============================] - 3s 875us/step - loss: 0.1220 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1264 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 89/100\n",
      "3774/3774 [==============================] - 3s 887us/step - loss: 0.1220 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1268 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 90/100\n",
      "3774/3774 [==============================] - 3s 890us/step - loss: 0.1219 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1286 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 91/100\n",
      "3774/3774 [==============================] - 3s 892us/step - loss: 0.1219 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1271 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 92/100\n",
      "3774/3774 [==============================] - 3s 889us/step - loss: 0.1218 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1265 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 93/100\n",
      "3774/3774 [==============================] - 3s 865us/step - loss: 0.1217 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1272 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 94/100\n",
      "3774/3774 [==============================] - 3s 873us/step - loss: 0.1217 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1266 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 95/100\n",
      "3774/3774 [==============================] - 3s 890us/step - loss: 0.1214 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1274 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 96/100\n",
      "3774/3774 [==============================] - 3s 889us/step - loss: 0.1216 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1260 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 97/100\n",
      "3774/3774 [==============================] - 3s 885us/step - loss: 0.1217 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1262 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 98/100\n",
      "3774/3774 [==============================] - 3s 889us/step - loss: 0.1214 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1249 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 99/100\n",
      "3774/3774 [==============================] - 3s 885us/step - loss: 0.1213 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1268 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 100/100\n",
      "3774/3774 [==============================] - 3s 883us/step - loss: 0.1214 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1256 - val_avg_multilabel_BA_2: 0.8653\n",
      "Best epoch: 100\n",
      "Epoch 1/100\n",
      "3774/3774 [==============================] - 5s 977us/step - loss: 0.2016 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1735 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 2/100\n",
      "3774/3774 [==============================] - 3s 891us/step - loss: 0.1677 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1603 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 3/100\n",
      "3774/3774 [==============================] - 3s 893us/step - loss: 0.1585 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1547 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 4/100\n",
      "3774/3774 [==============================] - 3s 893us/step - loss: 0.1540 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1517 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 5/100\n",
      "3774/3774 [==============================] - 3s 904us/step - loss: 0.1509 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1487 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 6/100\n",
      "3774/3774 [==============================] - 3s 897us/step - loss: 0.1485 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1479 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 7/100\n",
      "3774/3774 [==============================] - 3s 894us/step - loss: 0.1466 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1471 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 8/100\n",
      "3774/3774 [==============================] - 3s 897us/step - loss: 0.1451 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1445 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 9/100\n",
      "3774/3774 [==============================] - 3s 913us/step - loss: 0.1439 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1423 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 10/100\n",
      "3774/3774 [==============================] - 3s 896us/step - loss: 0.1428 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1445 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 11/100\n",
      "3774/3774 [==============================] - 3s 897us/step - loss: 0.1419 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1426 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 12/100\n",
      "3774/3774 [==============================] - 3s 895us/step - loss: 0.1411 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1415 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 13/100\n",
      "3774/3774 [==============================] - 3s 894us/step - loss: 0.1402 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1422 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 14/100\n",
      "3774/3774 [==============================] - 3s 892us/step - loss: 0.1396 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1406 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 15/100\n",
      "3774/3774 [==============================] - 3s 894us/step - loss: 0.1389 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1395 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 16/100\n",
      "3774/3774 [==============================] - 3s 893us/step - loss: 0.1383 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1398 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 17/100\n",
      "3774/3774 [==============================] - 3s 895us/step - loss: 0.1378 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1373 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 18/100\n",
      "3774/3774 [==============================] - 3s 899us/step - loss: 0.1372 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1379 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 19/100\n",
      "3774/3774 [==============================] - 3s 899us/step - loss: 0.1367 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1367 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 20/100\n",
      "3774/3774 [==============================] - 4s 957us/step - loss: 0.1361 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1376 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 21/100\n",
      "3774/3774 [==============================] - 4s 988us/step - loss: 0.1357 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1374 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 22/100\n",
      "3774/3774 [==============================] - 4s 972us/step - loss: 0.1353 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1392 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 23/100\n",
      "3774/3774 [==============================] - 4s 1ms/step - loss: 0.1350 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1381 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 24/100\n",
      "3774/3774 [==============================] - 4s 968us/step - loss: 0.1344 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1389 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 25/100\n",
      "3774/3774 [==============================] - 4s 960us/step - loss: 0.1342 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1365 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 26/100\n",
      "3774/3774 [==============================] - 4s 959us/step - loss: 0.1338 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1347 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 27/100\n",
      "3774/3774 [==============================] - 4s 966us/step - loss: 0.1334 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1372 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 28/100\n",
      "3774/3774 [==============================] - 3s 917us/step - loss: 0.1332 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1346 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 29/100\n",
      "3774/3774 [==============================] - 3s 903us/step - loss: 0.1328 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1355 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 30/100\n",
      "3774/3774 [==============================] - 3s 903us/step - loss: 0.1325 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1353 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 31/100\n",
      "3774/3774 [==============================] - 3s 899us/step - loss: 0.1325 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1373 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 32/100\n",
      "3774/3774 [==============================] - 3s 896us/step - loss: 0.1320 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1343 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 33/100\n",
      "3774/3774 [==============================] - 3s 905us/step - loss: 0.1318 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1345 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 34/100\n",
      "3774/3774 [==============================] - 4s 936us/step - loss: 0.1317 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1337 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 35/100\n",
      "3774/3774 [==============================] - 3s 907us/step - loss: 0.1315 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1342 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 36/100\n",
      "3774/3774 [==============================] - 4s 951us/step - loss: 0.1314 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1337 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 37/100\n",
      "3774/3774 [==============================] - 3s 912us/step - loss: 0.1310 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1330 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 38/100\n",
      "3774/3774 [==============================] - 3s 921us/step - loss: 0.1310 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1332 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 39/100\n",
      "3774/3774 [==============================] - 3s 901us/step - loss: 0.1306 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1337 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 40/100\n",
      "3774/3774 [==============================] - 3s 897us/step - loss: 0.1305 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1354 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 41/100\n",
      "3774/3774 [==============================] - 3s 903us/step - loss: 0.1304 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1330 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 42/100\n",
      "3774/3774 [==============================] - 3s 924us/step - loss: 0.1302 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1342 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 43/100\n",
      "3774/3774 [==============================] - 3s 889us/step - loss: 0.1300 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1318 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 44/100\n",
      "3774/3774 [==============================] - 3s 898us/step - loss: 0.1300 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1332 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 45/100\n",
      "3774/3774 [==============================] - 3s 893us/step - loss: 0.1297 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1331 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 46/100\n",
      "3774/3774 [==============================] - 3s 888us/step - loss: 0.1296 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1321 - val_avg_multilabel_BA_2: 0.8648\n",
      "Epoch 47/100\n",
      "3774/3774 [==============================] - 3s 888us/step - loss: 0.1294 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1319 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 48/100\n",
      "3774/3774 [==============================] - 3s 878us/step - loss: 0.1292 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1323 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 49/100\n",
      "3774/3774 [==============================] - 3s 904us/step - loss: 0.1291 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1321 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 50/100\n",
      "3774/3774 [==============================] - 3s 901us/step - loss: 0.1289 - avg_multilabel_BA_2: 0.8648 - val_loss: 0.1311 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 51/100\n",
      "3774/3774 [==============================] - 3s 891us/step - loss: 0.1289 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1327 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 52/100\n",
      "3774/3774 [==============================] - 3s 887us/step - loss: 0.1289 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1312 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 53/100\n",
      "3774/3774 [==============================] - 3s 877us/step - loss: 0.1288 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1311 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 54/100\n",
      "3774/3774 [==============================] - 3s 876us/step - loss: 0.1285 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1316 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 55/100\n",
      "3774/3774 [==============================] - 3s 873us/step - loss: 0.1284 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1303 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 56/100\n",
      "3774/3774 [==============================] - 3s 873us/step - loss: 0.1284 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1306 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 57/100\n",
      "3774/3774 [==============================] - 3s 877us/step - loss: 0.1283 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1321 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 58/100\n",
      "3774/3774 [==============================] - 3s 869us/step - loss: 0.1280 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1313 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 59/100\n",
      "3774/3774 [==============================] - 3s 872us/step - loss: 0.1281 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1301 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 60/100\n",
      "3774/3774 [==============================] - 3s 876us/step - loss: 0.1278 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1327 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 61/100\n",
      "3774/3774 [==============================] - 3s 875us/step - loss: 0.1278 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1306 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 62/100\n",
      "3774/3774 [==============================] - 3s 882us/step - loss: 0.1277 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1315 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 63/100\n",
      "3774/3774 [==============================] - 3s 878us/step - loss: 0.1276 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1301 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 64/100\n",
      "3774/3774 [==============================] - 3s 873us/step - loss: 0.1275 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1299 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 65/100\n",
      "3774/3774 [==============================] - 3s 872us/step - loss: 0.1274 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1309 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 66/100\n",
      "3774/3774 [==============================] - 3s 873us/step - loss: 0.1274 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1300 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 67/100\n",
      "3774/3774 [==============================] - 3s 900us/step - loss: 0.1272 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1301 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 68/100\n",
      "3774/3774 [==============================] - 3s 870us/step - loss: 0.1271 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1308 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 69/100\n",
      "3774/3774 [==============================] - 3s 875us/step - loss: 0.1272 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1314 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 70/100\n",
      "3774/3774 [==============================] - 3s 873us/step - loss: 0.1270 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1305 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 71/100\n",
      "3774/3774 [==============================] - 3s 878us/step - loss: 0.1269 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1296 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 72/100\n",
      "3774/3774 [==============================] - 3s 877us/step - loss: 0.1269 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1302 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 73/100\n",
      "3774/3774 [==============================] - 3s 887us/step - loss: 0.1267 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1303 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 74/100\n",
      "3774/3774 [==============================] - 3s 874us/step - loss: 0.1267 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1297 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 75/100\n",
      "3774/3774 [==============================] - 3s 875us/step - loss: 0.1267 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1298 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 76/100\n",
      "3774/3774 [==============================] - 3s 875us/step - loss: 0.1266 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1342 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 77/100\n",
      "3774/3774 [==============================] - 3s 876us/step - loss: 0.1265 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1318 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 78/100\n",
      "3774/3774 [==============================] - 3s 873us/step - loss: 0.1264 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1310 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 79/100\n",
      "3774/3774 [==============================] - 3s 870us/step - loss: 0.1263 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1300 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 80/100\n",
      "3774/3774 [==============================] - 3s 876us/step - loss: 0.1263 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1315 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 81/100\n",
      "3774/3774 [==============================] - 3s 881us/step - loss: 0.1261 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1290 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 82/100\n",
      "3774/3774 [==============================] - 3s 873us/step - loss: 0.1261 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1319 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 83/100\n",
      "3774/3774 [==============================] - 3s 875us/step - loss: 0.1261 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1303 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 84/100\n",
      "3774/3774 [==============================] - 3s 873us/step - loss: 0.1262 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1299 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 85/100\n",
      "3774/3774 [==============================] - 3s 872us/step - loss: 0.1259 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1294 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 86/100\n",
      "3774/3774 [==============================] - 3s 870us/step - loss: 0.1259 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1294 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 87/100\n",
      "3774/3774 [==============================] - 3s 884us/step - loss: 0.1258 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1290 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 88/100\n",
      "3774/3774 [==============================] - 3s 872us/step - loss: 0.1257 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1306 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 89/100\n",
      "3774/3774 [==============================] - 3s 873us/step - loss: 0.1257 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1289 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 90/100\n",
      "3774/3774 [==============================] - 3s 872us/step - loss: 0.1256 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1283 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 91/100\n",
      "3774/3774 [==============================] - 3s 876us/step - loss: 0.1255 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1312 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 92/100\n",
      "3774/3774 [==============================] - 3s 875us/step - loss: 0.1255 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1308 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 93/100\n",
      "3774/3774 [==============================] - 3s 870us/step - loss: 0.1254 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1295 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 94/100\n",
      "3774/3774 [==============================] - 3s 873us/step - loss: 0.1255 - avg_multilabel_BA_2: 0.8653 - val_loss: 0.1300 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 95/100\n",
      "3774/3774 [==============================] - 3s 874us/step - loss: 0.1254 - avg_multilabel_BA_2: 0.8653 - val_loss: 0.1306 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 96/100\n",
      "3774/3774 [==============================] - 3s 871us/step - loss: 0.1253 - avg_multilabel_BA_2: 0.8653 - val_loss: 0.1294 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 97/100\n",
      "3774/3774 [==============================] - 3s 875us/step - loss: 0.1253 - avg_multilabel_BA_2: 0.8653 - val_loss: 0.1300 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 98/100\n",
      "3774/3774 [==============================] - 3s 877us/step - loss: 0.1253 - avg_multilabel_BA_2: 0.8653 - val_loss: 0.1288 - val_avg_multilabel_BA_2: 0.8654\n",
      "Epoch 99/100\n",
      "3774/3774 [==============================] - 3s 871us/step - loss: 0.1251 - avg_multilabel_BA_2: 0.8653 - val_loss: 0.1288 - val_avg_multilabel_BA_2: 0.8654\n",
      "Epoch 100/100\n",
      "3774/3774 [==============================] - 3s 873us/step - loss: 0.1252 - avg_multilabel_BA_2: 0.8654 - val_loss: 0.1300 - val_avg_multilabel_BA_2: 0.8654\n",
      "1843/1843 [==============================] - 1s 548us/step - loss: 0.1302 - avg_multilabel_BA_2: 0.8654\n",
      "Test results - Loss: 0.13017775118350983 - Averaged Balanced Accuracy: 0.8653587698936462%\n",
      "Averaged Balanced Accuracy: 0.865376\n",
      "INFO:tensorflow:Assets written to: model/saved_model_fold_3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/saved_model_fold_3/assets\n",
      "2022-04-11 16:52:27.848650: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2022-04-11 16:52:27.848669: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "2022-04-11 16:52:27.848673: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:372] Ignored change_concat_input_ranges.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
      "2022-04-11 16:52:27.848845: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: model/saved_model_fold_3\n",
      "2022-04-11 16:52:27.849590: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-04-11 16:52:27.849602: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: model/saved_model_fold_3\n",
      "2022-04-11 16:52:27.851768: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-04-11 16:52:27.878506: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: model/saved_model_fold_3\n",
      "2022-04-11 16:52:27.887161: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 38317 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project hypertunning/experimento_1_RandSearchCV/oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project hypertunning/experimento_1_RandSearchCV/oracle.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from hypertunning/experimento_1_RandSearchCV/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from hypertunning/experimento_1_RandSearchCV/tuner0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3847/3847 [==============================] - 4s 948us/step - loss: 0.1978 - avg_multilabel_BA_2: 0.8653 - val_loss: 0.1741 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 2/100\n",
      "3847/3847 [==============================] - 3s 870us/step - loss: 0.1635 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1596 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 3/100\n",
      "3847/3847 [==============================] - 3s 901us/step - loss: 0.1549 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1537 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 4/100\n",
      "3847/3847 [==============================] - 4s 921us/step - loss: 0.1501 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1488 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 5/100\n",
      "3847/3847 [==============================] - 3s 900us/step - loss: 0.1468 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1467 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 6/100\n",
      "3847/3847 [==============================] - 4s 917us/step - loss: 0.1441 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1440 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 7/100\n",
      "3847/3847 [==============================] - 3s 908us/step - loss: 0.1419 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1423 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 8/100\n",
      "3847/3847 [==============================] - 3s 906us/step - loss: 0.1403 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1423 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 9/100\n",
      "3847/3847 [==============================] - 3s 890us/step - loss: 0.1390 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1394 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 10/100\n",
      "3847/3847 [==============================] - 3s 891us/step - loss: 0.1377 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1369 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 11/100\n",
      "3847/3847 [==============================] - 3s 891us/step - loss: 0.1366 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1363 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 12/100\n",
      "3847/3847 [==============================] - 3s 889us/step - loss: 0.1358 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1377 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 13/100\n",
      "3847/3847 [==============================] - 3s 889us/step - loss: 0.1347 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1353 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 14/100\n",
      "3847/3847 [==============================] - 3s 892us/step - loss: 0.1339 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1352 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 15/100\n",
      "3847/3847 [==============================] - 4s 918us/step - loss: 0.1331 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1338 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 16/100\n",
      "3847/3847 [==============================] - 4s 945us/step - loss: 0.1325 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1344 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 17/100\n",
      "3847/3847 [==============================] - 4s 966us/step - loss: 0.1317 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1326 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 18/100\n",
      "3847/3847 [==============================] - 4s 936us/step - loss: 0.1312 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1317 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 19/100\n",
      "3847/3847 [==============================] - 4s 939us/step - loss: 0.1307 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1345 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 20/100\n",
      "3847/3847 [==============================] - 4s 933us/step - loss: 0.1302 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1339 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 21/100\n",
      "3847/3847 [==============================] - 4s 957us/step - loss: 0.1297 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1312 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 22/100\n",
      "3847/3847 [==============================] - 4s 962us/step - loss: 0.1291 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1333 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 23/100\n",
      "3847/3847 [==============================] - 4s 986us/step - loss: 0.1288 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1317 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 24/100\n",
      "3847/3847 [==============================] - 4s 955us/step - loss: 0.1285 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1294 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 25/100\n",
      "3847/3847 [==============================] - 4s 934us/step - loss: 0.1281 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1307 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 26/100\n",
      "3847/3847 [==============================] - 4s 949us/step - loss: 0.1275 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1303 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 27/100\n",
      "3847/3847 [==============================] - 4s 947us/step - loss: 0.1273 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1285 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 28/100\n",
      "3847/3847 [==============================] - 4s 945us/step - loss: 0.1270 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1294 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 29/100\n",
      "3847/3847 [==============================] - 4s 954us/step - loss: 0.1267 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1279 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 30/100\n",
      "3847/3847 [==============================] - 4s 966us/step - loss: 0.1264 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1289 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 31/100\n",
      "3847/3847 [==============================] - 4s 944us/step - loss: 0.1261 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1296 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 32/100\n",
      "3847/3847 [==============================] - 4s 951us/step - loss: 0.1259 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1300 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 33/100\n",
      "3847/3847 [==============================] - 4s 922us/step - loss: 0.1256 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1299 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 34/100\n",
      "3847/3847 [==============================] - 3s 893us/step - loss: 0.1254 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1290 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 35/100\n",
      "3847/3847 [==============================] - 3s 891us/step - loss: 0.1252 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1280 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 36/100\n",
      "3847/3847 [==============================] - 3s 892us/step - loss: 0.1249 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1280 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 37/100\n",
      "3847/3847 [==============================] - 3s 892us/step - loss: 0.1248 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1263 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 38/100\n",
      "3847/3847 [==============================] - 3s 893us/step - loss: 0.1247 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1275 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 39/100\n",
      "3847/3847 [==============================] - 3s 898us/step - loss: 0.1243 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1260 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 40/100\n",
      "3847/3847 [==============================] - 3s 883us/step - loss: 0.1241 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1265 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 41/100\n",
      "3847/3847 [==============================] - 3s 891us/step - loss: 0.1239 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1265 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 42/100\n",
      "3847/3847 [==============================] - 3s 892us/step - loss: 0.1238 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1274 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 43/100\n",
      "3847/3847 [==============================] - 3s 890us/step - loss: 0.1238 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1263 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 44/100\n",
      "3847/3847 [==============================] - 3s 891us/step - loss: 0.1236 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1272 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 45/100\n",
      "3847/3847 [==============================] - 3s 883us/step - loss: 0.1234 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1264 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 46/100\n",
      "3847/3847 [==============================] - 3s 889us/step - loss: 0.1234 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1254 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 47/100\n",
      "3847/3847 [==============================] - 3s 890us/step - loss: 0.1232 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1255 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 48/100\n",
      "3847/3847 [==============================] - 3s 888us/step - loss: 0.1230 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1249 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 49/100\n",
      "3847/3847 [==============================] - 3s 901us/step - loss: 0.1228 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1247 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 50/100\n",
      "3847/3847 [==============================] - 3s 890us/step - loss: 0.1227 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1256 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 51/100\n",
      "3847/3847 [==============================] - 3s 895us/step - loss: 0.1225 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1261 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 52/100\n",
      "3847/3847 [==============================] - 3s 890us/step - loss: 0.1224 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1273 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 53/100\n",
      "3847/3847 [==============================] - 3s 887us/step - loss: 0.1222 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1246 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 54/100\n",
      "3847/3847 [==============================] - 3s 888us/step - loss: 0.1223 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1258 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 55/100\n",
      "3847/3847 [==============================] - 3s 887us/step - loss: 0.1219 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1244 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 56/100\n",
      "3847/3847 [==============================] - 3s 889us/step - loss: 0.1220 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1261 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 57/100\n",
      "3847/3847 [==============================] - 3s 888us/step - loss: 0.1218 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1242 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 58/100\n",
      "3847/3847 [==============================] - 3s 892us/step - loss: 0.1219 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1269 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 59/100\n",
      "3847/3847 [==============================] - 3s 894us/step - loss: 0.1217 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1244 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 60/100\n",
      "3847/3847 [==============================] - 3s 889us/step - loss: 0.1216 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1256 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 61/100\n",
      "3847/3847 [==============================] - 3s 887us/step - loss: 0.1214 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1236 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 62/100\n",
      "3847/3847 [==============================] - 3s 888us/step - loss: 0.1214 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1245 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 63/100\n",
      "3847/3847 [==============================] - 3s 890us/step - loss: 0.1211 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1233 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 64/100\n",
      "3847/3847 [==============================] - 3s 890us/step - loss: 0.1211 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1232 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 65/100\n",
      "3847/3847 [==============================] - 3s 887us/step - loss: 0.1211 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1247 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 66/100\n",
      "3847/3847 [==============================] - 3s 889us/step - loss: 0.1209 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1254 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 67/100\n",
      "3847/3847 [==============================] - 3s 891us/step - loss: 0.1207 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1262 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 68/100\n",
      "3847/3847 [==============================] - 3s 887us/step - loss: 0.1208 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1255 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 69/100\n",
      "3847/3847 [==============================] - 3s 892us/step - loss: 0.1207 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1238 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 70/100\n",
      "3847/3847 [==============================] - 3s 890us/step - loss: 0.1206 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1248 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 71/100\n",
      "3847/3847 [==============================] - 3s 892us/step - loss: 0.1205 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1235 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 72/100\n",
      "3847/3847 [==============================] - 3s 887us/step - loss: 0.1205 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1240 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 73/100\n",
      "3847/3847 [==============================] - 3s 891us/step - loss: 0.1202 - avg_multilabel_BA_2: 0.8653 - val_loss: 0.1230 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 74/100\n",
      "3847/3847 [==============================] - 3s 891us/step - loss: 0.1201 - avg_multilabel_BA_2: 0.8653 - val_loss: 0.1253 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 75/100\n",
      "3847/3847 [==============================] - 3s 890us/step - loss: 0.1201 - avg_multilabel_BA_2: 0.8653 - val_loss: 0.1239 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 76/100\n",
      "3847/3847 [==============================] - 3s 888us/step - loss: 0.1200 - avg_multilabel_BA_2: 0.8653 - val_loss: 0.1233 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 77/100\n",
      "3847/3847 [==============================] - 3s 892us/step - loss: 0.1199 - avg_multilabel_BA_2: 0.8653 - val_loss: 0.1238 - val_avg_multilabel_BA_2: 0.8654\n",
      "Epoch 78/100\n",
      "3847/3847 [==============================] - 3s 896us/step - loss: 0.1199 - avg_multilabel_BA_2: 0.8653 - val_loss: 0.1236 - val_avg_multilabel_BA_2: 0.8654\n",
      "Epoch 79/100\n",
      "3847/3847 [==============================] - 3s 886us/step - loss: 0.1198 - avg_multilabel_BA_2: 0.8654 - val_loss: 0.1235 - val_avg_multilabel_BA_2: 0.8654\n",
      "Epoch 80/100\n",
      "3847/3847 [==============================] - 3s 886us/step - loss: 0.1197 - avg_multilabel_BA_2: 0.8654 - val_loss: 0.1223 - val_avg_multilabel_BA_2: 0.8654\n",
      "Epoch 81/100\n",
      "3847/3847 [==============================] - 3s 888us/step - loss: 0.1197 - avg_multilabel_BA_2: 0.8654 - val_loss: 0.1229 - val_avg_multilabel_BA_2: 0.8654\n",
      "Epoch 82/100\n",
      "3847/3847 [==============================] - 3s 887us/step - loss: 0.1196 - avg_multilabel_BA_2: 0.8654 - val_loss: 0.1238 - val_avg_multilabel_BA_2: 0.8654\n",
      "Epoch 83/100\n",
      "3847/3847 [==============================] - 3s 884us/step - loss: 0.1195 - avg_multilabel_BA_2: 0.8654 - val_loss: 0.1254 - val_avg_multilabel_BA_2: 0.8654\n",
      "Epoch 84/100\n",
      "3847/3847 [==============================] - 3s 888us/step - loss: 0.1194 - avg_multilabel_BA_2: 0.8655 - val_loss: 0.1239 - val_avg_multilabel_BA_2: 0.8654\n",
      "Epoch 85/100\n",
      "3847/3847 [==============================] - 3s 889us/step - loss: 0.1195 - avg_multilabel_BA_2: 0.8655 - val_loss: 0.1239 - val_avg_multilabel_BA_2: 0.8654\n",
      "Epoch 86/100\n",
      "3847/3847 [==============================] - 3s 890us/step - loss: 0.1194 - avg_multilabel_BA_2: 0.8655 - val_loss: 0.1226 - val_avg_multilabel_BA_2: 0.8655\n",
      "Epoch 87/100\n",
      "3847/3847 [==============================] - 3s 889us/step - loss: 0.1193 - avg_multilabel_BA_2: 0.8655 - val_loss: 0.1217 - val_avg_multilabel_BA_2: 0.8655\n",
      "Epoch 88/100\n",
      "3847/3847 [==============================] - 3s 885us/step - loss: 0.1192 - avg_multilabel_BA_2: 0.8655 - val_loss: 0.1239 - val_avg_multilabel_BA_2: 0.8655\n",
      "Epoch 89/100\n",
      "3847/3847 [==============================] - 3s 888us/step - loss: 0.1191 - avg_multilabel_BA_2: 0.8655 - val_loss: 0.1239 - val_avg_multilabel_BA_2: 0.8655\n",
      "Epoch 90/100\n",
      "3847/3847 [==============================] - 3s 887us/step - loss: 0.1190 - avg_multilabel_BA_2: 0.8655 - val_loss: 0.1235 - val_avg_multilabel_BA_2: 0.8655\n",
      "Epoch 91/100\n",
      "3847/3847 [==============================] - 3s 893us/step - loss: 0.1190 - avg_multilabel_BA_2: 0.8655 - val_loss: 0.1324 - val_avg_multilabel_BA_2: 0.8655\n",
      "Epoch 92/100\n",
      "3847/3847 [==============================] - 3s 886us/step - loss: 0.1189 - avg_multilabel_BA_2: 0.8655 - val_loss: 0.1235 - val_avg_multilabel_BA_2: 0.8655\n",
      "Epoch 93/100\n",
      "3847/3847 [==============================] - 3s 889us/step - loss: 0.1188 - avg_multilabel_BA_2: 0.8655 - val_loss: 0.1256 - val_avg_multilabel_BA_2: 0.8655\n",
      "Epoch 94/100\n",
      "3847/3847 [==============================] - 3s 893us/step - loss: 0.1188 - avg_multilabel_BA_2: 0.8655 - val_loss: 0.1228 - val_avg_multilabel_BA_2: 0.8655\n",
      "Epoch 95/100\n",
      "3847/3847 [==============================] - 3s 889us/step - loss: 0.1189 - avg_multilabel_BA_2: 0.8655 - val_loss: 0.1231 - val_avg_multilabel_BA_2: 0.8656\n",
      "Epoch 96/100\n",
      "3847/3847 [==============================] - 3s 901us/step - loss: 0.1187 - avg_multilabel_BA_2: 0.8655 - val_loss: 0.1218 - val_avg_multilabel_BA_2: 0.8656\n",
      "Epoch 97/100\n",
      "3847/3847 [==============================] - 4s 913us/step - loss: 0.1185 - avg_multilabel_BA_2: 0.8655 - val_loss: 0.1251 - val_avg_multilabel_BA_2: 0.8656\n",
      "Epoch 98/100\n",
      "3847/3847 [==============================] - 3s 904us/step - loss: 0.1185 - avg_multilabel_BA_2: 0.8656 - val_loss: 0.1231 - val_avg_multilabel_BA_2: 0.8656\n",
      "Epoch 99/100\n",
      "3847/3847 [==============================] - 3s 891us/step - loss: 0.1186 - avg_multilabel_BA_2: 0.8656 - val_loss: 0.1250 - val_avg_multilabel_BA_2: 0.8656\n",
      "Epoch 100/100\n",
      "3847/3847 [==============================] - 3s 889us/step - loss: 0.1185 - avg_multilabel_BA_2: 0.8656 - val_loss: 0.1238 - val_avg_multilabel_BA_2: 0.8656\n",
      "Best epoch: 100\n",
      "Epoch 1/100\n",
      "3847/3847 [==============================] - 4s 965us/step - loss: 0.2008 - avg_multilabel_BA_2: 0.8656 - val_loss: 0.1748 - val_avg_multilabel_BA_2: 0.8655\n",
      "Epoch 2/100\n",
      "3847/3847 [==============================] - 3s 881us/step - loss: 0.1657 - avg_multilabel_BA_2: 0.8655 - val_loss: 0.1613 - val_avg_multilabel_BA_2: 0.8655\n",
      "Epoch 3/100\n",
      "3847/3847 [==============================] - 3s 872us/step - loss: 0.1574 - avg_multilabel_BA_2: 0.8655 - val_loss: 0.1553 - val_avg_multilabel_BA_2: 0.8654\n",
      "Epoch 4/100\n",
      "3847/3847 [==============================] - 3s 876us/step - loss: 0.1528 - avg_multilabel_BA_2: 0.8654 - val_loss: 0.1528 - val_avg_multilabel_BA_2: 0.8654\n",
      "Epoch 5/100\n",
      "3847/3847 [==============================] - 3s 876us/step - loss: 0.1500 - avg_multilabel_BA_2: 0.8654 - val_loss: 0.1485 - val_avg_multilabel_BA_2: 0.8654\n",
      "Epoch 6/100\n",
      "3847/3847 [==============================] - 3s 886us/step - loss: 0.1481 - avg_multilabel_BA_2: 0.8653 - val_loss: 0.1473 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 7/100\n",
      "3847/3847 [==============================] - 3s 871us/step - loss: 0.1466 - avg_multilabel_BA_2: 0.8653 - val_loss: 0.1467 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 8/100\n",
      "3847/3847 [==============================] - 3s 881us/step - loss: 0.1452 - avg_multilabel_BA_2: 0.8653 - val_loss: 0.1446 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 9/100\n",
      "3847/3847 [==============================] - 3s 890us/step - loss: 0.1441 - avg_multilabel_BA_2: 0.8653 - val_loss: 0.1444 - val_avg_multilabel_BA_2: 0.8653\n",
      "Epoch 10/100\n",
      "3847/3847 [==============================] - 3s 865us/step - loss: 0.1432 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1453 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 11/100\n",
      "3847/3847 [==============================] - 3s 876us/step - loss: 0.1423 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1428 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 12/100\n",
      "3847/3847 [==============================] - 3s 873us/step - loss: 0.1416 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1418 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 13/100\n",
      "3847/3847 [==============================] - 3s 878us/step - loss: 0.1410 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1414 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 14/100\n",
      "3847/3847 [==============================] - 3s 876us/step - loss: 0.1403 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1415 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 15/100\n",
      "3847/3847 [==============================] - 3s 872us/step - loss: 0.1399 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1416 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 16/100\n",
      "3847/3847 [==============================] - 3s 863us/step - loss: 0.1394 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1396 - val_avg_multilabel_BA_2: 0.8652\n",
      "Epoch 17/100\n",
      "3847/3847 [==============================] - 3s 842us/step - loss: 0.1390 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1390 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 18/100\n",
      "3847/3847 [==============================] - 3s 843us/step - loss: 0.1385 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1395 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 19/100\n",
      "3847/3847 [==============================] - 3s 847us/step - loss: 0.1382 - avg_multilabel_BA_2: 0.8652 - val_loss: 0.1397 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 20/100\n",
      "3847/3847 [==============================] - 3s 838us/step - loss: 0.1379 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1383 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 21/100\n",
      "3847/3847 [==============================] - 3s 845us/step - loss: 0.1375 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1399 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 22/100\n",
      "3847/3847 [==============================] - 3s 844us/step - loss: 0.1372 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1385 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 23/100\n",
      "3847/3847 [==============================] - 3s 846us/step - loss: 0.1367 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1387 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 24/100\n",
      "3847/3847 [==============================] - 3s 841us/step - loss: 0.1366 - avg_multilabel_BA_2: 0.8651 - val_loss: 0.1374 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 25/100\n",
      "3847/3847 [==============================] - 3s 844us/step - loss: 0.1362 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1371 - val_avg_multilabel_BA_2: 0.8651\n",
      "Epoch 26/100\n",
      "3847/3847 [==============================] - 3s 843us/step - loss: 0.1358 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1371 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 27/100\n",
      "3847/3847 [==============================] - 3s 845us/step - loss: 0.1356 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1362 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 28/100\n",
      "3847/3847 [==============================] - 3s 844us/step - loss: 0.1353 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1359 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 29/100\n",
      "3847/3847 [==============================] - 3s 842us/step - loss: 0.1351 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1376 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 30/100\n",
      "3847/3847 [==============================] - 3s 842us/step - loss: 0.1347 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1356 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 31/100\n",
      "3847/3847 [==============================] - 3s 844us/step - loss: 0.1344 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1356 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 32/100\n",
      "3847/3847 [==============================] - 3s 844us/step - loss: 0.1343 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1349 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 33/100\n",
      "3847/3847 [==============================] - 3s 843us/step - loss: 0.1340 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1356 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 34/100\n",
      "3847/3847 [==============================] - 3s 844us/step - loss: 0.1338 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1358 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 35/100\n",
      "3847/3847 [==============================] - 3s 843us/step - loss: 0.1335 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1337 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 36/100\n",
      "3847/3847 [==============================] - 3s 844us/step - loss: 0.1333 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1342 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 37/100\n",
      "3847/3847 [==============================] - 3s 848us/step - loss: 0.1332 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1346 - val_avg_multilabel_BA_2: 0.8650\n",
      "Epoch 38/100\n",
      "3847/3847 [==============================] - 3s 844us/step - loss: 0.1328 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1361 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 39/100\n",
      "3847/3847 [==============================] - 3s 848us/step - loss: 0.1328 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1340 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 40/100\n",
      "3847/3847 [==============================] - 3s 841us/step - loss: 0.1326 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1354 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 41/100\n",
      "3847/3847 [==============================] - 3s 842us/step - loss: 0.1324 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1343 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 42/100\n",
      "3847/3847 [==============================] - 3s 846us/step - loss: 0.1323 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1329 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 43/100\n",
      "3847/3847 [==============================] - 3s 849us/step - loss: 0.1320 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1354 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 44/100\n",
      "3847/3847 [==============================] - 3s 840us/step - loss: 0.1320 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1333 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 45/100\n",
      "3847/3847 [==============================] - 3s 846us/step - loss: 0.1317 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1332 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 46/100\n",
      "3847/3847 [==============================] - 3s 847us/step - loss: 0.1316 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1336 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 47/100\n",
      "3847/3847 [==============================] - 3s 839us/step - loss: 0.1314 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1327 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 48/100\n",
      "3847/3847 [==============================] - 3s 845us/step - loss: 0.1314 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1339 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 49/100\n",
      "3847/3847 [==============================] - 3s 841us/step - loss: 0.1312 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1343 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 50/100\n",
      "3847/3847 [==============================] - 3s 842us/step - loss: 0.1310 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1340 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 51/100\n",
      "3847/3847 [==============================] - 3s 842us/step - loss: 0.1309 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1332 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 52/100\n",
      "3847/3847 [==============================] - 3s 842us/step - loss: 0.1309 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1331 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 53/100\n",
      "3847/3847 [==============================] - 3s 843us/step - loss: 0.1307 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1320 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 54/100\n",
      "3847/3847 [==============================] - 3s 839us/step - loss: 0.1306 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1382 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 55/100\n",
      "3847/3847 [==============================] - 3s 870us/step - loss: 0.1305 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1322 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 56/100\n",
      "3847/3847 [==============================] - 3s 875us/step - loss: 0.1304 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1327 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 57/100\n",
      "3847/3847 [==============================] - 3s 870us/step - loss: 0.1302 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1311 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 58/100\n",
      "3847/3847 [==============================] - 3s 870us/step - loss: 0.1301 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1322 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 59/100\n",
      "3847/3847 [==============================] - 3s 872us/step - loss: 0.1300 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1327 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 60/100\n",
      "3847/3847 [==============================] - 3s 844us/step - loss: 0.1299 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1310 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 61/100\n",
      "3847/3847 [==============================] - 3s 849us/step - loss: 0.1298 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1315 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 62/100\n",
      "3847/3847 [==============================] - 3s 865us/step - loss: 0.1297 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1311 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 63/100\n",
      "3847/3847 [==============================] - 3s 868us/step - loss: 0.1295 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1365 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 64/100\n",
      "3847/3847 [==============================] - 3s 870us/step - loss: 0.1294 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1323 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 65/100\n",
      "3847/3847 [==============================] - 3s 867us/step - loss: 0.1293 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1330 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 66/100\n",
      "3847/3847 [==============================] - 3s 866us/step - loss: 0.1292 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1319 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 67/100\n",
      "3847/3847 [==============================] - 3s 851us/step - loss: 0.1290 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1317 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 68/100\n",
      "3847/3847 [==============================] - 3s 849us/step - loss: 0.1290 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1313 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 69/100\n",
      "3847/3847 [==============================] - 3s 848us/step - loss: 0.1288 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1319 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 70/100\n",
      "3847/3847 [==============================] - 3s 863us/step - loss: 0.1287 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1303 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 71/100\n",
      "3847/3847 [==============================] - 3s 857us/step - loss: 0.1286 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1322 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 72/100\n",
      "3847/3847 [==============================] - 3s 845us/step - loss: 0.1286 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1330 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 73/100\n",
      "3847/3847 [==============================] - 3s 840us/step - loss: 0.1284 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1429 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 74/100\n",
      "3847/3847 [==============================] - 3s 844us/step - loss: 0.1284 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1321 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 75/100\n",
      "3847/3847 [==============================] - 3s 846us/step - loss: 0.1283 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1306 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 76/100\n",
      "3847/3847 [==============================] - 3s 842us/step - loss: 0.1283 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1300 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 77/100\n",
      "3847/3847 [==============================] - 3s 841us/step - loss: 0.1281 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1309 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 78/100\n",
      "3847/3847 [==============================] - 3s 838us/step - loss: 0.1280 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1322 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 79/100\n",
      "3847/3847 [==============================] - 3s 843us/step - loss: 0.1279 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1321 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 80/100\n",
      "3847/3847 [==============================] - 3s 846us/step - loss: 0.1278 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1305 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 81/100\n",
      "3847/3847 [==============================] - 3s 847us/step - loss: 0.1278 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1310 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 82/100\n",
      "3847/3847 [==============================] - 3s 848us/step - loss: 0.1279 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1296 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 83/100\n",
      "3847/3847 [==============================] - 3s 846us/step - loss: 0.1276 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1299 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 84/100\n",
      "3847/3847 [==============================] - 3s 847us/step - loss: 0.1276 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1285 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 85/100\n",
      "3847/3847 [==============================] - 3s 845us/step - loss: 0.1275 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1332 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 86/100\n",
      "3847/3847 [==============================] - 3s 841us/step - loss: 0.1273 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1293 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 87/100\n",
      "3847/3847 [==============================] - 3s 843us/step - loss: 0.1274 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1292 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 88/100\n",
      "3847/3847 [==============================] - 3s 848us/step - loss: 0.1274 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1302 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 89/100\n",
      "3847/3847 [==============================] - 3s 850us/step - loss: 0.1273 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1304 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 90/100\n",
      "3847/3847 [==============================] - 3s 845us/step - loss: 0.1272 - avg_multilabel_BA_2: 0.8649 - val_loss: 0.1286 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 91/100\n",
      "3847/3847 [==============================] - 3s 848us/step - loss: 0.1271 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1299 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 92/100\n",
      "3847/3847 [==============================] - 3s 845us/step - loss: 0.1270 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1292 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 93/100\n",
      "3847/3847 [==============================] - 3s 842us/step - loss: 0.1270 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1319 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 94/100\n",
      "3847/3847 [==============================] - 3s 842us/step - loss: 0.1269 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1295 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 95/100\n",
      "3847/3847 [==============================] - 3s 842us/step - loss: 0.1268 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1306 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 96/100\n",
      "3847/3847 [==============================] - 3s 842us/step - loss: 0.1268 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1282 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 97/100\n",
      "3847/3847 [==============================] - 3s 848us/step - loss: 0.1267 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1301 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 98/100\n",
      "3847/3847 [==============================] - 3s 843us/step - loss: 0.1267 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1299 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 99/100\n",
      "3847/3847 [==============================] - 3s 845us/step - loss: 0.1267 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1309 - val_avg_multilabel_BA_2: 0.8649\n",
      "Epoch 100/100\n",
      "3847/3847 [==============================] - 3s 845us/step - loss: 0.1266 - avg_multilabel_BA_2: 0.8650 - val_loss: 0.1310 - val_avg_multilabel_BA_2: 0.8650\n",
      "1879/1879 [==============================] - 1s 554us/step - loss: 0.1316 - avg_multilabel_BA_2: 0.8650\n",
      "Test results - Loss: 0.13159377872943878 - Averaged Balanced Accuracy: 0.8649806976318359%\n",
      "Averaged Balanced Accuracy: 0.864958\n",
      "INFO:tensorflow:Assets written to: model/saved_model_fold_4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/saved_model_fold_4/assets\n",
      "2022-04-11 17:03:56.106726: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8625562, 0.86519814, 0.86450756, 0.86537576, 0.8649584]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-11 17:03:56.106745: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "2022-04-11 17:03:56.106750: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:372] Ignored change_concat_input_ranges.\n",
      "2022-04-11 17:03:56.106925: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: model/saved_model_fold_4\n",
      "2022-04-11 17:03:56.107694: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-04-11 17:03:56.107706: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: model/saved_model_fold_4\n",
      "2022-04-11 17:03:56.109878: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-04-11 17:03:56.136549: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: model/saved_model_fold_4\n",
      "2022-04-11 17:03:56.145208: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 38284 microseconds.\n"
     ]
    }
   ],
   "source": [
    "bas = []\n",
    "for fold in k_folds:\n",
    "    config = {\n",
    "        'df_path': k_folds[fold]['40'],\n",
    "        'labels': labels\n",
    "    }\n",
    "    \n",
    "    har = utils.HAR(config)\n",
    "    #har.run()\n",
    "    model, best_hps, best_epoch, test_results, ba = har.hypertunning()\n",
    "    bas.append(ba.numpy())\n",
    "    \n",
    "    har.mlp.model.save(f'model/saved_model_{fold}')\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(f'model/saved_model_{fold}') # path to the SavedModel directory\n",
    "    tflite_model = converter.convert()\n",
    "    with open(f'model/model_base_{fold}.tflite', 'wb') as f:\n",
    "      f.write(tflite_model)\n",
    "\n",
    "print(bas)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27832af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_acc:magnitude_stats:mean</th>\n",
       "      <th>raw_acc:magnitude_stats:std</th>\n",
       "      <th>raw_acc:magnitude_stats:moment3</th>\n",
       "      <th>raw_acc:magnitude_stats:moment4</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile25</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile50</th>\n",
       "      <th>raw_acc:magnitude_stats:percentile75</th>\n",
       "      <th>raw_acc:magnitude_stats:value_entropy</th>\n",
       "      <th>raw_acc:magnitude_stats:time_entropy</th>\n",
       "      <th>raw_acc:magnitude_spectrum:log_energy_band0</th>\n",
       "      <th>...</th>\n",
       "      <th>lf_measurements:temperature_ambient</th>\n",
       "      <th>discrete:time_of_day:between0and6</th>\n",
       "      <th>discrete:time_of_day:between3and9</th>\n",
       "      <th>discrete:time_of_day:between6and12</th>\n",
       "      <th>discrete:time_of_day:between9and15</th>\n",
       "      <th>discrete:time_of_day:between12and18</th>\n",
       "      <th>discrete:time_of_day:between15and21</th>\n",
       "      <th>discrete:time_of_day:between18and24</th>\n",
       "      <th>discrete:time_of_day:between21and3</th>\n",
       "      <th>label_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.057536</td>\n",
       "      <td>0.040597</td>\n",
       "      <td>-0.048977</td>\n",
       "      <td>0.124759</td>\n",
       "      <td>1.053158</td>\n",
       "      <td>1.057091</td>\n",
       "      <td>1.060935</td>\n",
       "      <td>0.344809</td>\n",
       "      <td>6.683838</td>\n",
       "      <td>5.043598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.057436</td>\n",
       "      <td>0.006165</td>\n",
       "      <td>-0.009415</td>\n",
       "      <td>0.018645</td>\n",
       "      <td>1.055086</td>\n",
       "      <td>1.057279</td>\n",
       "      <td>1.060143</td>\n",
       "      <td>1.014093</td>\n",
       "      <td>6.684595</td>\n",
       "      <td>5.042748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.056344</td>\n",
       "      <td>0.006302</td>\n",
       "      <td>-0.004635</td>\n",
       "      <td>0.013525</td>\n",
       "      <td>1.053282</td>\n",
       "      <td>1.056208</td>\n",
       "      <td>1.059165</td>\n",
       "      <td>1.429112</td>\n",
       "      <td>6.684594</td>\n",
       "      <td>5.043642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.056874</td>\n",
       "      <td>0.004767</td>\n",
       "      <td>-0.002796</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>1.053958</td>\n",
       "      <td>1.057010</td>\n",
       "      <td>1.059996</td>\n",
       "      <td>2.190168</td>\n",
       "      <td>6.684602</td>\n",
       "      <td>5.043075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.057353</td>\n",
       "      <td>0.005415</td>\n",
       "      <td>0.006585</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>1.054161</td>\n",
       "      <td>1.057100</td>\n",
       "      <td>1.059976</td>\n",
       "      <td>1.827865</td>\n",
       "      <td>6.684599</td>\n",
       "      <td>5.043392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 226 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   raw_acc:magnitude_stats:mean  raw_acc:magnitude_stats:std  \\\n",
       "0                      1.057536                     0.040597   \n",
       "1                      1.057436                     0.006165   \n",
       "2                      1.056344                     0.006302   \n",
       "3                      1.056874                     0.004767   \n",
       "4                      1.057353                     0.005415   \n",
       "\n",
       "   raw_acc:magnitude_stats:moment3  raw_acc:magnitude_stats:moment4  \\\n",
       "0                        -0.048977                         0.124759   \n",
       "1                        -0.009415                         0.018645   \n",
       "2                        -0.004635                         0.013525   \n",
       "3                        -0.002796                         0.007088   \n",
       "4                         0.006585                         0.010781   \n",
       "\n",
       "   raw_acc:magnitude_stats:percentile25  raw_acc:magnitude_stats:percentile50  \\\n",
       "0                              1.053158                              1.057091   \n",
       "1                              1.055086                              1.057279   \n",
       "2                              1.053282                              1.056208   \n",
       "3                              1.053958                              1.057010   \n",
       "4                              1.054161                              1.057100   \n",
       "\n",
       "   raw_acc:magnitude_stats:percentile75  \\\n",
       "0                              1.060935   \n",
       "1                              1.060143   \n",
       "2                              1.059165   \n",
       "3                              1.059996   \n",
       "4                              1.059976   \n",
       "\n",
       "   raw_acc:magnitude_stats:value_entropy  \\\n",
       "0                               0.344809   \n",
       "1                               1.014093   \n",
       "2                               1.429112   \n",
       "3                               2.190168   \n",
       "4                               1.827865   \n",
       "\n",
       "   raw_acc:magnitude_stats:time_entropy  \\\n",
       "0                              6.683838   \n",
       "1                              6.684595   \n",
       "2                              6.684594   \n",
       "3                              6.684602   \n",
       "4                              6.684599   \n",
       "\n",
       "   raw_acc:magnitude_spectrum:log_energy_band0  ...  \\\n",
       "0                                     5.043598  ...   \n",
       "1                                     5.042748  ...   \n",
       "2                                     5.043642  ...   \n",
       "3                                     5.043075  ...   \n",
       "4                                     5.043392  ...   \n",
       "\n",
       "   lf_measurements:temperature_ambient  discrete:time_of_day:between0and6  \\\n",
       "0                                  0.0                                0.0   \n",
       "1                                  0.0                                0.0   \n",
       "2                                  0.0                                0.0   \n",
       "3                                  0.0                                0.0   \n",
       "4                                  0.0                                0.0   \n",
       "\n",
       "   discrete:time_of_day:between3and9  discrete:time_of_day:between6and12  \\\n",
       "0                                0.0                                 0.0   \n",
       "1                                0.0                                 0.0   \n",
       "2                                0.0                                 0.0   \n",
       "3                                0.0                                 0.0   \n",
       "4                                0.0                                 0.0   \n",
       "\n",
       "   discrete:time_of_day:between9and15  discrete:time_of_day:between12and18  \\\n",
       "0                                 1.0                                  1.0   \n",
       "1                                 1.0                                  1.0   \n",
       "2                                 1.0                                  1.0   \n",
       "3                                 1.0                                  1.0   \n",
       "4                                 1.0                                  1.0   \n",
       "\n",
       "   discrete:time_of_day:between15and21  discrete:time_of_day:between18and24  \\\n",
       "0                                  0.0                                  0.0   \n",
       "1                                  0.0                                  0.0   \n",
       "2                                  0.0                                  0.0   \n",
       "3                                  0.0                                  0.0   \n",
       "4                                  0.0                                  0.0   \n",
       "\n",
       "   discrete:time_of_day:between21and3  label_source  \n",
       "0                                 0.0             2  \n",
       "1                                 0.0             2  \n",
       "2                                 0.0             2  \n",
       "3                                 0.0             2  \n",
       "4                                 0.0             2  \n",
       "\n",
       "[5 rows x 226 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "har.data.x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4be0888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_38 (Dense)            (None, 44)                9988      \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 6)                 270       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,258\n",
      "Trainable params: 10,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "har.mlp.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb1d79",
   "metadata": {},
   "source": [
    "Com os modelos salvos, o restante do experimento segue em ambiente Android"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
